{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import cycle\n",
    "import os\n",
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import subprocess\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from importlib import reload\n",
    "# os.chdir(\"PATH TO WHERE YOU GIT CLONED\") ## Make sure your kernel is in the same directory as the regional_library.py file\n",
    "import regional_library as ml\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does this package do?\n",
    "\n",
    "Setting up a regional model in MOM6 is a pain. The goal of this package is that users should spend their debugging time fixing a model that's running and doing weird things, rather than puzzling over a model that won't even start.\n",
    "\n",
    "In running this notebook, you'll hopefully have a running mom6 regional model. There will still be a lot of fiddling to do with the MOM_input file to make sure that the parameters are set up right for your domain, and you might want to manually edit some of the input files. BUT, this package should help you bypass most of the woes of regridding, encoding and understanding the arcane arts of the mom6 boundary segment files. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook is designed to showcase where we're up to so far. By the end you should have a running mom6 experiment on the domain of your choice. To make a stable test case:\n",
    "\n",
    "* Avoid any regions with ice\n",
    "* Avoid regions near the north pole\n",
    "* Although the default configuration is meant to be RYF, I've not fixed up the calendar and encoding to run longer than a year just yet\n",
    "* If you choose to do OM2-01 forcing, set your start date to 1990-01-01 which is what I've got it hardcoded to in step 2 option 2. \n",
    "\n",
    "Also hgrid is currently **not** mercator. It's equally spaced lat/long, with square cells at the centre of your domain and decrease in cell area away from equator. The pipeline is modular however, so if another hgrid generation function is written, it will be easy to pass say a gridtype=\"mercator\" to the experiment class.\n",
    "\n",
    "Input Type | Source\n",
    "---|---\n",
    "Surface | JRA (or ERA5 - see end of notebook)\n",
    "Ocean | GLORYS reanalysis product OR ACCESS OM2-01\n",
    "Bathymetry | Gebco"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Your personal environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch = \"/scratch/v45/ab8992\"\n",
    "home = \"/home/149/ab8992\"\n",
    "## If using GLORYs, you'll need an email and password to access their database. make an account here: https://www.copernicus.eu/en/user/login?\n",
    "pwd = \"YOUR COPERNICUS PASSWORD\"    \n",
    "usr = \"YOUR COPERNICUS USERNAME\"  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Choose our domain, define workspace paths\n",
    "\n",
    "To make sure that things are working I'd recommend starting with the default example defined below. If this runs ok, then change to a domain of your choice and hopefully it runs ok too! There's some troubleshooting you can do if not (check readme / readthedocs)\n",
    "\n",
    "To find the lat/lon of the domain you want to test you can use <a href=\"https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/download\" > this GUI </a> and copy paste below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_name = \"tasmania-20thdeg\"\n",
    "\n",
    "## Choose your coordinates and the name of your experiment\n",
    "yextent = [-48,-38.95] ## latitude\n",
    "xextent = [143,150] ## longitude\n",
    "\n",
    "daterange = [\"2003-01-01 00:00:00\", \"2003-01-05 00:00:00\"] ## 2003 is a good compimise for GLORYs and JRA forcing as they overlap. JRA ends in 2012, GLORYS starts in 1993\n",
    "\n",
    "## Place where all your input files go\n",
    "inputdir = f\"{scratch}/mom6_regional_configs/{expt_name}/\"\n",
    "\n",
    "## Directory where you'll run the experiment from\n",
    "rundir = f\"{home}/mom6_rundirs/{expt_name}/\"\n",
    "\n",
    "## Directory where fre tools are stored\n",
    "toolpath = \"/home/157/ahg157/repos/mom5/src/tools/\" ## Compiled tools needed for construction of mask tables\n",
    "\n",
    "## Directory where raw downloads go before processing\n",
    "tmpdir = f\"{scratch}/regional_tmp/{expt_name}\"\n",
    "\n",
    "for i in [rundir,tmpdir,inputdir]:\n",
    "    if not os.path.exists(i):\n",
    "        subprocess.run(f\"mkdir {i} -p\",shell=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Prepare ocean forcing data\n",
    "\n",
    "We need to cut out our ocean forcing. The pipeline expects an initial condition and one time-dependent segment per non-land boundary. Naming convention is \"east_unprocessed\" and \"ic_unprocessed\" for initial condition. Execute either of the following cells to pick GLORYs reanalysis or ACCESS OM2-01"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For default 'Tassie' domain:\n",
    "You can just read in the boundaries I've already downloaded. Overwrite your tmpdir and continue with the notebook without generating ocean forcing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir = \"/g/data/v45/ab8992/tassie-glorys\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option1: GLORYs\n",
    "The following cell generates a bash script in your designated 'temporary directory'. This should be on scratch somewhere and just a container for your raw donloads.\n",
    "\n",
    "To do this you'll need to register with the Copernicus data centre to get a username and password. Fill these in below.\n",
    "\n",
    "After executing, navigate to this directory in your terminal and double check that all the files are there! Sometimes the data centre hangs and only retrieves a couple of files. In thise case, comment out the completed segments in `get_oceanfiles.sh` and run it again from terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file = open(f\"{tmpdir}/get_oceanfiles.sh\",\"w\")\n",
    "file.write(\n",
    "        ml.motu_requests(xextent, yextent, daterange, tmpdir, usr, pwd,[\"north\",\"south\",\"east\",\"west\"])\n",
    ")\n",
    "file.close()\n",
    "\n",
    "subprocess.run(\n",
    "    f\"bash {tmpdir}/get_oceanfiles.sh\",shell=True\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: ACCESS OM2-01\n",
    "\n",
    "If you have access to where it's located on Gadi, you can execute the following cell to cut out and save your segments and use these instead. The default I've set it at below is to cut out 3 months. To cut out a year, uncomment the code above which concatenates several input files together. Keep in mind that these input files are HUGE and they'll take a while to open and processes. To do a whole year, you'll want to run with a whole node and go make yourself a cup of coffee (and maybe read the paper for a bit). \n",
    "\n",
    "The advantage of doing this though is that the input files that the pipeline has to deal with are a lot smaller, making subsequent computation a lot quicker. An older iteration of the boundary brushcutter was to read data directly from the huge datasets, but this required some very careful chunking to not break your kernel. \n",
    "\n",
    "**NOTE: I haven't automated this properly and it's hardcoded for the year of 1990, which corresponds to files 1077 - 1082. Could maybe use COSIMA cookbook for this step instead?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########## TWO OPTIONS: ############################\n",
    "\n",
    "## Use this if you want to do a quick test for up to 3 months\n",
    "om2_input = xr.open_mfdataset(f\"/g/data/ik11/outputs/access-om2-01/01deg_jra55v13_ryf9091/output1080/ocean/ocean_daily*\",parallel=True,chunks='auto').sel(time=slice(daterange[0],daterange[1]))\n",
    "## Use this to cut out entire year \n",
    "# om2_input = xr.concat(\n",
    "#     [xr.open_mfdataset(f\"/g/data/ik11/outputs/access-om2-01/01deg_jra55v13_ryf9091/output{i}/ocean/ocean_daily*\",decode_times = False,parallel=True,chunks='auto') for i in range(1077,1082)],\n",
    "#     \"time\"\n",
    "# )\n",
    "#!  for i in range(1077,1082) is hardcoded to choose the year of 1990 Jan -> Dec 31. \n",
    "#######################################################\n",
    "\n",
    "## Cut out initial condition and save\n",
    "ic = om2_input[[\"u\",\"v\",\"salt\",\"temp\",\"eta_t\"]].sel(    \n",
    "    yu_ocean = slice(yextent[0] - 0.2,yextent[1] + 0.2),\n",
    "    yt_ocean = slice(yextent[0] - 0.2,yextent[1] + 0.2)\n",
    ").isel(time = 0)\n",
    "\n",
    "## Nicer Slicer handles seams in longitude and different grids. Ensures that the output matches our 'xextend'\n",
    "ic = ml.nicer_slicer(ic,[xextent[0],xextent[1]],[\"xu_ocean\",\"xt_ocean\"])\n",
    "ic.to_netcdf(tmpdir + \"/ic_unprocessed\")\n",
    "\n",
    "## Cut out East and West segments. Does lat slice first then uses nicer slicer for lon slice\n",
    "eastwest = om2_input[[\"u\",\"v\",\"salt\",\"temp\",\"eta_t\"]].sel(    \n",
    "    yu_ocean = slice(yextent[0] - 0.2,yextent[1] + 0.2),\n",
    "    yt_ocean = slice(yextent[0] - 0.2,yextent[1] + 0.2)\n",
    ")\n",
    "ml.nicer_slicer(eastwest,[xextent[1],xextent[1]],[\"xu_ocean\",\"xt_ocean\"]).to_netcdf(tmpdir + \"/east_unprocessed\")\n",
    "ml.nicer_slicer(eastwest,[xextent[0],xextent[0]],[\"xu_ocean\",\"xt_ocean\"]).to_netcdf(tmpdir + \"/west_unprocessed\")\n",
    "\n",
    "## Cut out North and South segments\n",
    "northsouth = ml.nicer_slicer(om2_input[[\"u\",\"v\",\"salt\",\"temp\",\"eta_t\"]],[xextent[0],xextent[1]],[\"xu_ocean\",\"xt_ocean\"])\n",
    "northsouth.sel(\n",
    "    yu_ocean = slice(yextent[1] - 0.2,yextent[1] + 0.2),\n",
    "    yt_ocean = slice(yextent[1] - 0.2,yextent[1] + 0.2)\n",
    ").to_netcdf(tmpdir + \"/north_unprocessed\")\n",
    "northsouth.sel(\n",
    "    yu_ocean = slice(yextent[0] - 0.2,yextent[0] + 0.2),\n",
    "    yt_ocean = slice(yextent[0] - 0.2,yextent[0] + 0.2)\n",
    ").to_netcdf(tmpdir + \"/south_unprocessed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Make experiment object\n",
    "This object keeps track of your domain basics, as well as generating the hgrid, vgrid and setting up the folder structures. \n",
    "\n",
    "After running you can have a look at your grids by calling `expt.hgrid` and `expt.vgrid`\n",
    "\n",
    "Plotting vgrid with marker = '.' option lets you see the spacing, or plotting \n",
    "```python\n",
    "np.diff(expt.hgrid.zl).plot(marker = '.')\n",
    "```\n",
    " shows you the vertical spacing profile.\n",
    "\n",
    "## Modular workflow!\n",
    "\n",
    "After constructing your expt object, if you don't like my lazy default hgrid and vgrid you can simply modify and overwrite them. However, you'll also need to save them to disk again as I've not automated this just yet. For example:\n",
    "\n",
    "```python\n",
    "expt.hgrid = custom_hgrid\n",
    "expt.hgrid.to_netcdf(f\"{inputdir}/hgrid.nc\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ml)\n",
    "expt = ml.experiment(\n",
    "    xextent,\n",
    "    yextent,\n",
    "    daterange,\n",
    "    0.05,  # Resolution\n",
    "    75,    # Number of vertical layers\n",
    "    10,    # Ratio of largest to smallest vertical layer. Select 1 for linear, negative number for higher resolution at bottom\n",
    "    4500,  # Depth of simulation\n",
    "    rundir,\n",
    "    inputdir,\n",
    "    toolpath\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Set up bathymetry\n",
    "\n",
    "Similarly to ocean forcing, we point our 'bathymetry' method at the location of the file of choice, and pass it a dictionary mapping variable names. This time we don't need to preprocess the topography since it's just a 2D field and easier to deal with. Afterwards you can run `expt.topog` and have a look at your domain. After running this cell, your input directory will contain other topography - adjacent things like the ocean mosaic and mask table too. This defaults to a 10x10 layout which can be updated later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.bathymetry(\n",
    "    '/g/data/ik11/inputs/GEBCO_2022/GEBCO_2022.nc',\n",
    "    {\"xh\":\"lon\",\n",
    "     \"yh\":\"lat\",\n",
    "     \"elevation\":\"elevation\"}, ## Again this dictionary just maps mom6 variable names to what they are in your topog.\n",
    "     minimum_layers = 1\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out your domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.topog.depth.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Step 5: Handle the ocean forcing - where the magic happens\n",
    "\n",
    "This cuts out and interpolates the initial condition as well as all boundaries (unless you don't pass it boundaries).\n",
    "\n",
    "The dictionary maps the mom6 variable names to what they're called in your ocean input file. Notice how for GLORYs, the horizontal dimensions are x and y, vs xh, yh, xq, yq for ACCESS OM2-01. This is because for an 'A' grid type tracers share the grid with velocities so there's no difference.\n",
    "\n",
    "If one of your segments is land, you can delete its string from the 'boundaries' list. You'll need to update MOM_input to reflect this though so it knows how many segments to look for, and their orientations. \n",
    "\n",
    "### **Note: Only run one of the two cells below according to what forcing you chose!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR GLORYS: \n",
    "expt.ocean_forcing(\n",
    "    tmpdir,  ## Path to ocean foring files\n",
    "    {\"time\":\"time\",\n",
    "     \"y\":\"latitude\",\n",
    "     \"x\":\"longitude\",\n",
    "     \"zl\":\"depth\",\n",
    "     \"eta\":\"zos\",\n",
    "     \"u\":\"uo\",\n",
    "     \"v\":\"vo\",\n",
    "     \"tracers\":{\"salt\":\"so\",\n",
    "                \"temp\":\"thetao\"\n",
    "                }\n",
    "    },\n",
    "    boundaries = [\"south\",\"north\",\"west\",\"east\"],\n",
    "    gridtype=\"A\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR ACCESS OM2: \n",
    "expt.ocean_forcing(\n",
    "    tmpdir,  ## Path to ocean foring files\n",
    "    {\"time\":\"time\",\n",
    "     \"yh\":\"yt_ocean\",\n",
    "     \"xh\":\"xt_ocean\",\n",
    "     \"xq\":\"xu_ocean\",\n",
    "     \"yq\":\"yu_ocean\",\n",
    "     \"zl\":\"st_ocean\",\n",
    "     \"eta\":\"eta_t\",\n",
    "     \"u\":\"u\",\n",
    "     \"v\":\"v\",\n",
    "     \"tracers\":{\"salt\":\"salt\",\"temp\":\"temp\"}},\n",
    "    boundaries = [\"south\",\"north\",\"west\",\"east\"],\n",
    "    gridtype=\"B\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 (optional) Select number of processors \n",
    "\n",
    "This is just a wrapper for check_mask FRE tool. Choose the number of processors in the X and Y directions respectively. Having run `.bathymetry()`, you already have one set up for 10x10 by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.processor_mask((10,10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 Regrid the runoff \n",
    "\n",
    " This step will be removed in a future update when this functionality is added to rest of pipeline. Currently it calls a function from the legacy regional_model_scripts file. Just execute cell to give your domain runoff from JRA in 1991. Rivers do the same thing every year right?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regional_model_scripts import regrid_runoff\n",
    "runoff_path = \"/g/data/ik11/inputs/JRA-55/RYF/v1-3/RYF.runoff_all.1990_1991.nc\" ## Can change to match your year\n",
    "\n",
    "regrid_runoff(inputdir + \"ocean_mask.nc\",\n",
    "    inputdir + \"hgrid.nc\",\n",
    "    runoff_path,\n",
    "    inputdir + \"runoff_regrid.nc\",\n",
    "    np.array(xextent) - np.array([180,180]),\n",
    "    yextent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Modify the default input directory to make a (hopefully) runnable configuration out of the box\n",
    "\n",
    "This cell just copies a default run directory and modifies it to match your configuration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(f\"cp default_rundir/jra_surface/* {rundir} -r\",shell = True)\n",
    "# subprocess.run(f\"cp default_rundir/era5_surface/* {rundir} -r\",shell = True)\n",
    "subprocess.run(f\"ln -s {inputdir} {rundir}/inputdir\",shell=True)\n",
    "\n",
    "hgrid = xr.open_dataset(f\"{inputdir}/hgrid.nc\")\n",
    "\n",
    "## Get mask table information\n",
    "ncpus = 10\n",
    "mask_table = None\n",
    "for i in os.listdir(f\"{inputdir}\"):\n",
    "    if \"mask_table\" in i:\n",
    "        mask_table = i\n",
    "        a = mask_table.split(\".\")[1]\n",
    "        b = mask_table.split(\".\")[2].split(\"x\")\n",
    "        ncpus = int(b[0]) * int(b[1]) - int(a)\n",
    "\n",
    "\n",
    "## Modify MOM_input\n",
    "inputfile = open(f\"{rundir}/MOM_input\",'r')\n",
    "lines = inputfile.readlines()\n",
    "inputfile.close()\n",
    "for i in range(len(lines)):\n",
    "    if \"MASKTABLE\" in lines[i]:\n",
    "        if mask_table != None:\n",
    "            lines[i] = f'MASKTABLE = \"{mask_table}\"\\n'\n",
    "        else:\n",
    "            lines[i] = \"# MASKTABLE = no mask table\"\n",
    "    if \"LAYOUT =\" in lines[i] and \"IO\" not in lines[i]:\n",
    "        lines[i] = f'LAYOUT = {expt.layout[1]},{expt.layout[0]}\\n'\n",
    "\n",
    "    if \"NIGLOBAL\" in lines[i]: \n",
    "        # lines[i] = f\"NIGLOBAL = {str(x_indices_centre[1] - x_indices_centre[0])}\\n\"\n",
    "        lines[i] = f\"NIGLOBAL = {hgrid.nx.shape[0]//2}\\n\"\n",
    "\n",
    "        \n",
    "    if \"NJGLOBAL\" in lines[i]:\n",
    "        # lines[i] = f\"NJGLOBAL = {str(y_indices_centre[1] - y_indices_centre[0])}\\n\"\n",
    "        lines[i] = f\"NJGLOBAL = {hgrid.ny.shape[0]//2}\\n\"\n",
    "\n",
    "        \n",
    "inputfile = open(f\"{rundir}/MOM_input\",'w')\n",
    "\n",
    "inputfile.writelines(lines)\n",
    "inputfile.close()\n",
    "\n",
    "## Modify SIS_input\n",
    "inputfile = open(f\"{rundir}/SIS_input\",'r')\n",
    "lines = inputfile.readlines()\n",
    "inputfile.close()\n",
    "for i in range(len(lines)):\n",
    "    if \"MASKTABLE\" in lines[i]:\n",
    "        lines[i] = f'MASKTABLE = \"{mask_table}\"\\n'\n",
    "    if \"NIGLOBAL\" in lines[i]:\n",
    "        # lines[i] = f\"NIGLOBAL = {str(x_indices_centre[1] - x_indices_centre[0])}\\n\"\n",
    "        lines[i] = f\"NIGLOBAL = {hgrid.nx.shape[0]//2}\\n\"\n",
    "    if \"LAYOUT =\" in lines[i] and \"IO\" not in lines[i]:\n",
    "        lines[i] = f'LAYOUT = {expt.layout[1]},{expt.layout[0]}\\n'\n",
    "    if \"NJGLOBAL\" in lines[i]:\n",
    "        # lines[i] = f\"NJGLOBAL = {str(y_indices_centre[1] - y_indices_centre[0])}\\n\"\n",
    "        lines[i] = f\"NJGLOBAL = {hgrid.ny.shape[0]//2}\\n\"\n",
    "        \n",
    "inputfile = open(f\"{rundir}/SIS_input\",'w')\n",
    "inputfile.writelines(lines)\n",
    "inputfile.close()\n",
    "\n",
    "## Modify config.yaml \n",
    "inputfile = open(f\"{rundir}/config.yaml\",'r')\n",
    "lines = inputfile.readlines()\n",
    "inputfile.close()\n",
    "for i in range(len(lines)):\n",
    "    if \"ncpus\" in lines[i]:\n",
    "        lines[i] = f'ncpus: {str(ncpus)}\\n'\n",
    "    if \"jobname\" in lines[i]:\n",
    "        lines[i] = f\"jobname: mom6_{expt_name}\\n\"\n",
    "        \n",
    "    if \"input:\" in lines[i]:\n",
    "        lines[i + 1] = f\"    - {inputdir}\\n\"\n",
    "\n",
    "inputfile = open(f\"{rundir}/config.yaml\",'w')\n",
    "inputfile.writelines(lines)\n",
    "inputfile.close()\n",
    "\n",
    "\n",
    "# Modify input.nml \n",
    "inputfile = open(f\"{rundir}/input.nml\",'r')\n",
    "lines = inputfile.readlines()\n",
    "inputfile.close()\n",
    "for i in range(len(lines)):\n",
    "    if \"current_date\" in lines[i]:\n",
    "        tmp = daterange[0].split(\" \")[0].split(\"-\")\n",
    "        lines[i] = f\"{lines[i].split(' = ')[0]} = {int(tmp[0])},{int(tmp[1])},{int(tmp[2])},0,0,0,\\n\"\n",
    "\n",
    " \n",
    "inputfile = open(f\"{rundir}/input.nml\",'w')\n",
    "inputfile.writelines(lines)\n",
    "inputfile.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS! Want to use ERA5 surface forcing instead?\n",
    "\n",
    "This is WIP and not well tested but thought I'd include it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SET UP ERA5 forcing:\n",
    "Here we take the ERA forcing as it already exists on Gadi. For NCI users, you need access to the rt group. ERA5 - specific functions provided cut out the region of interest and fix up the metadata ready for MOM6.\n",
    "\n",
    "For this example, we are forcing for the entire year of 2015 so we just generate a single forcing file with 2015's data.\n",
    "\n",
    "Below is a table showing ERA5 characteristics and what needs to be done to sort it out\n",
    "### Required ERA data:\n",
    "Name | ERA filename | era variable name | notes\n",
    "---|---|---|---\n",
    "Surface Pressure | sp | sp | Pa :heavy_check_mark:\n",
    "Surface Temperature | 2t | t2m | K :heavy_check_mark:\n",
    "Meridional Wind | 10v | v10 | m/s :heavy_check_mark:\n",
    "Zonal Wind | 10u | u10 | m/s :heavy_check_mark:\n",
    "Specific Humidity | na | na | kg/kg, calculated from dewpoint temperature\n",
    "Dewpoint Temperature | 2d | d2m | K\n",
    "\n",
    "\n",
    "We can calculate specific humidity $q$ from dewpoint temperature $T_d$ and surface pressure $P$ via saturation vapour pressure $P_v$.\n",
    "\n",
    "$\\large P_v = 10^{8.07131 - \\frac{1730.63}{233.426 + T}} \\frac{101325}{760} $ Pascals\n",
    "\n",
    "$\\large q = 0.001 * 0.622  \\frac{P_v}{P}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erapath = \"/g/data/rt52/era5/single-levels/reanalysis\"\n",
    "\n",
    "## Firstly just open all raw data\n",
    "rawdata = {}\n",
    "for fname , vname in zip([\"2t\",\"10u\",\"10v\",\"sp\",\"2d\"] , [\"t2m\",\"u10\",\"v10\",\"sp\",\"d2m\"]):\n",
    "\n",
    "    ## Cut out this variable to our domain size\n",
    "    rawdata[fname] = ml.nicer_slicer(\n",
    "        xr.open_mfdataset(f\"{erapath}/{fname}/{daterange[0].split('-')[0]}/{fname}*\",decode_times = False,chunks = {\"longitude\":100,\"latitude\":100}),\n",
    "        xextent,\n",
    "        \"longitude\"\n",
    "    ).sel(\n",
    "        latitude = slice(yextent[1],yextent[0]) ## This is because ERA5 has latitude in decreasing order (??)\n",
    "    )\n",
    "\n",
    "    ## Now fix up the latitude and time dimensions\n",
    "\n",
    "    rawdata[fname] = rawdata[fname].isel(\n",
    "        latitude = slice(None,None,-1) ## Flip latitude        \n",
    "        ).assign_coords(\n",
    "        time = np.arange(0,rawdata[fname].time.shape[0],dtype=float) ## Set the zero date of forcing to start of run\n",
    "        )\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    rawdata[fname].time.attrs = {\"calendar\":\"julian\",\"units\":f\"hours since {daterange[0]}\"} ## Fix up calendar to match\n",
    "\n",
    "    if fname == \"2d\":\n",
    "        ## Calculate specific humidity from dewpoint temperature \n",
    "        q = xr.Dataset(\n",
    "            data_vars= {\n",
    "                \"q\": (0.622 / rawdata[\"sp\"][\"sp\"]) * (10**(8.07131 - 1730.63 / (233.426 + rawdata[\"2d\"][\"d2m\"] - 273.15) )) * 101325 / 760\n",
    "                }\n",
    "\n",
    "        )\n",
    "        q.q.attrs = {\"long_name\":\"Specific Humidity\",\"units\": \"kg/kg\"}\n",
    "        q.to_netcdf(f\"{inputdir}/forcing/q_ERA5\",unlimited_dims = \"time\",encoding = {\"q\":{\"dtype\":\"double\"}})\n",
    "    else:\n",
    "        rawdata[fname].to_netcdf(f\"{inputdir}/forcing/{fname}_ERA5\",unlimited_dims = \"time\",encoding = {vname:{\"dtype\":\"double\"}})\n",
    "\n",
    "\n",
    "## Update the data table to match:\n",
    "\n",
    "subprocess.run(f\"cp default_rundir/era5_surface/data_table {rundir}/data_table\",shell = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3]",
   "language": "python",
   "name": "conda-env-analysis3-py"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
