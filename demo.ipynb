{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import cycle\n",
    "import os\n",
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import subprocess\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from importlib import reload\n",
    "os.chdir(\"/home/149/ab8992/cosima_regional/mom6-regional-scripts\")\n",
    "import regional_library as ml\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of Regional Pipeline\n",
    "\n",
    "This notebook is designed to showcase where we're up to so far. By the end you should have a running mom6 experiment on the domain of your choice. To make a stable test case:\n",
    "\n",
    "* Keep your domain fairly small (my test case is a rectangle around Tasmania). If you go bigger you'll need to do some FRE tool shenannigens as explained in step 5 to get it working.  \n",
    "* Avoid any regions with ice\n",
    "* Avoid regions near the north pole\n",
    "* Although the default configuration is meant to be RYF, I've not fixed up the calendar and encoding to run longer than a year just yet\n",
    "* If you choose to do OM2-01 forcing, set your start date to 1990-01-01 which is what I've got it hardcoded to in step 2 option 2. \n",
    "\n",
    "Also hgrid is currently **not** mercator. It's equally spaced lat/long. To be updated very soon.\n",
    "\n",
    "\n",
    "Input Type | Source\n",
    "---|---\n",
    "Surface | JRA\n",
    "Ocean | GLORYS reanalysis product OR ACCESS OM2-01\n",
    "Bathymetry | Gebco"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Choose our domain, define workspace paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ml)\n",
    "expt_name = \"democosima\"\n",
    "\n",
    "## Choose your coordinates and the name of your experiment\n",
    "yextent = [31,38]  ## latitude\n",
    "xextent = [33, 34] ## longitude\n",
    "\n",
    "daterange = [\"2003-01-01 00:00:00\", \"2003-01-05 00:00:00\"]\n",
    "\n",
    "## Place where all your input files go\n",
    "inputdir = f\"/scratch/v45/ab8992/mom6/regional_configs/{expt_name}/\"\n",
    "\n",
    "## Directory where you'll run the experiment from\n",
    "rundir = f\"/home/149/ab8992/mom6_rundirs/{expt_name}/\"\n",
    "\n",
    "## Directory where fre tools are stored\n",
    "toolpath = \"/home/157/ahg157/repos/mom5/src/tools/\" ## Compiled tools needed for construction of mask tables\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Prepare ocean forcing data\n",
    "\n",
    "We need to cut out our ocean forcing. The pipeline expects an initial condition and one time-dependent segment per non-land boundary. Naming convention is \"east_unprocessed\" and \"ic_inprocessed\" for initial condition. Execute either of the following cells to pick GLORYs reanalysis or ACCESS OM2-01"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option1: GLORYs\n",
    "The following cell generates a bash script in your designated 'temporary directory'. This should be on scratch somewhere and just a container for your raw donloads.\n",
    "\n",
    "To do this you'll need to register with the Copernicus data centre to get a username and password. Fill these in below.\n",
    "\n",
    "After executing, navigate to this directory in your terminal and run 'bash get_oceanfiles.sh'. Wait for all of your forcing segments to appear before continuing with the 'ocean forcing' step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Directory where raw downloads go before processing\n",
    "tmpdir = f\"/scratch/v45/ab8992/regional_tmp/{expt_name}/\"\n",
    "\n",
    "pwd = \"YOUR COPERNICUS PASSWORD\"    \n",
    "usr = \"YOUR COPERNICUS USERNAME\"    \n",
    "\n",
    "file = open(f\"{tmpdir}/get_oceanfiles.sh\",\"w\")\n",
    "file.write(\n",
    "        ml.motu_requests(xextent, yextent, daterange, tmpdir, usr, pwd,[\"north\",\"south\",\"east\",\"west\"])\n",
    ")\n",
    "file.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: ACCESS OM2-01\n",
    "\n",
    "If you have access to where it's located on Gadi, you can execute the following cell to cut out and save your segments and use these instead.\n",
    "\n",
    "**NOTE: I haven't automated this properly. You'll need to fiddle around with the 'for i in range(1077,1082)' line to choose the right year. Could maybe use COSIMA cookbook for this step instead?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir = f\"/scratch/v45/ab8992/regional_tmp/{expt_name}/\"\n",
    "\n",
    "om2_input = xr.concat(\n",
    "    [xr.open_mfdataset(f\"/g/data/ik11/outputs/access-om2-01/01deg_jra55v13_ryf9091/output{i}/ocean/ocean_daily*\",decode_times = False,parallel=True,chunks='auto') for i in range(1077,1082)]\n",
    ")\n",
    "#!  for i in range(1077,1082) is hardcoded to choose the year of 1990 Jan -> Dec 31. \n",
    "\n",
    "reload(ml)\n",
    "## Cut out initial condition and save\n",
    "ic = om2_input[[\"u\",\"v\",\"salt\",\"temp\",\"eta_t\"]].sel(    \n",
    "    yu_ocean = slice(yextent[0] - 0.2,yextent[1] + 0.2),\n",
    "    yt_ocean = slice(yextent[0] - 0.2,yextent[1] + 0.2)\n",
    ").isel(time = 0)\n",
    "\n",
    "## Nicer Slicer handles seams in longitude and different grids. Ensures that the output matches our 'xextend'\n",
    "ic = ml.nicer_slicer(ic,[xextent[0],xextent[1]],[\"xu_ocean\",\"xt_ocean\"])\n",
    "ic.to_netcdf(tmpdir + \"/ic_unprocessed\")\n",
    "\n",
    "eastwest = om2_input[[\"u\",\"v\",\"salt\",\"temp\",\"eta_t\"]].sel(    \n",
    "    yu_ocean = slice(yextent[0] - 0.2,yextent[1] + 0.2),\n",
    "    yt_ocean = slice(yextent[0] - 0.2,yextent[1] + 0.2)\n",
    ")\n",
    "ml.nicer_slicer(eastwest,[xextent[1],xextent[1]],[\"xu_ocean\",\"xt_ocean\"]).to_netcdf(tmpdir + \"/east_unprocessed\")\n",
    "ml.nicer_slicer(eastwest,[xextent[0],xextent[0]],[\"xu_ocean\",\"xt_ocean\"]).to_netcdf(tmpdir + \"/west_unprocessed\")\n",
    "\n",
    "northsouth = ml.nicer_slicer(om2_input[[\"u\",\"v\",\"salt\",\"temp\",\"eta_t\"]],[xextent[0],xextent[1]],[\"xu_ocean\",\"xt_ocean\"])\n",
    "northsouth.sel(\n",
    "    yu_ocean = slice(yextent[1] - 0.2,yextent[1] + 0.2),\n",
    "    yt_ocean = slice(yextent[1] - 0.2,yextent[1] + 0.2)\n",
    ").to_netcdf(tmpdir + \"/north_unprocessed\")\n",
    "northsouth.sel(\n",
    "    yu_ocean = slice(yextent[0] - 0.2,yextent[0] + 0.2),\n",
    "    yt_ocean = slice(yextent[0] - 0.2,yextent[0] + 0.2)\n",
    ").to_netcdf(tmpdir + \"/south_unprocessed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Make experiment object\n",
    "This object keeps track of your domain basics, as well as generating the hgrid, vgrid and setting up the folder structures. \n",
    "\n",
    "After running you can have a look at your grids by calling expt.hgrid and expt.vgrid\n",
    "\n",
    "Plotting vgrid with marker = '.' option lets you see the spacing, or plotting np.diff(expt.hgrid.zl).plot(marker = '.') shows you the vertical spacing profile.\n",
    "\n",
    "## Modular workflow!\n",
    "\n",
    "After constructing your expt object, if you don't like my lazy default hgrid and vgrid you can simply modify and overwrite them. However, you'll also need to save them to disk again as I've not automated this just yet. For example:\n",
    "\n",
    "expt.hgrid = custom_hgrid\n",
    "expt.hgrid.to_netcdf(f\"{inputdir}/hgrid.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt = ml.experiment(\n",
    "    xextent,\n",
    "    yextent,\n",
    "    daterange,\n",
    "    0.05,  # Resolution\n",
    "    75,    # Number of vertical layers\n",
    "    10,    # Ratio of largest to smallest vertical layer. Select 1 for linear, negative number for higher resolution at bottom\n",
    "    4500,  # Depth of simulation\n",
    "    rundir,\n",
    "    inputdir,\n",
    "    toolpath\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Step 4: Handle the ocean forcing.\n",
    "\n",
    "This cuts out and interpolates the initial condition as well as all boundaries (unless you don't pass it boundaries).\n",
    "\n",
    "The dictionary maps the mom6 variable names to what they're called in your ocean input file. Notice how for GLORYs, the horizontal dimensions are x and y, vs xh, yh, xq, yq for ACCESS OM2-01. This is because for an 'A' grid type tracers share the grid with velocities so there's no difference.\n",
    "\n",
    "If one of your segments is land, you can delete its string from the 'boundaries' list. You'll need to update MOM_input to reflect this though so it knows how many segments to look for, and their orientations. \n",
    "\n",
    "### **Note: Only run one of the two cells below according to what forcing you chose!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR GLORYS: \n",
    "expt.ocean_forcing(\n",
    "    tmpdir,  ## Path to ocean foring files\n",
    "    {\"time\":\"time\",\n",
    "     \"y\":\"latitude\",\n",
    "     \"x\":\"longitude\",\n",
    "     \"zl\":\"depth\",\n",
    "     \"eta\":\"zos\",\n",
    "     \"u\":\"uo\",\n",
    "     \"v\":\"vo\",\n",
    "     \"tracers\":{\"salt\":\"so\",\n",
    "                \"temp\":\"thetao\"\n",
    "                }\n",
    "    },\n",
    "    boundaries = [\"south\",\"north\",\"west\",\"east\"],\n",
    "    gridtype=\"A\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR ACCESS OM2: \n",
    "expt.ocean_forcing(\n",
    "    tmpdir,  ## Path to ocean foring files\n",
    "    {\"time\":\"time\",\n",
    "    \"y\":\"latitude\",\n",
    "    \"x\":\"longitude\",\n",
    "    \"zl\":\"depth\",\n",
    "    \"eta\":\"zos\",\n",
    "    \"u\":\"uo\",\n",
    "    \"v\":\"vo\",\n",
    "     \"tracers\":{\"salt\":\"so\",\n",
    "                \"temp\":\"thetao\"\n",
    "                }},\n",
    "    boundaries = [\"south\",\"north\",\"west\",\"east\"] \n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Set up bathymetry\n",
    "\n",
    "### **This is the most annoying step!** Will fix in future update\n",
    "Currently my library calls 'make_topog_parallel' as a subprocess. For large domains, this will simply hang as it won't have enough memory. If it doesn't print after 3 mins just cancel it. In this case you need to go to your input directory, start an interactive job and run:\n",
    "\n",
    "PATH_TO_EXECUTABLE/make_topog_parallel --mosaic ocean_mosaic.nc --topog_type realistic --topog_file bathy_original.nc --topog_field 'elevation' --scale_factor -1 --output topog_raw.nc\n",
    "\n",
    "A path to the executable is /g/data/v45/jr5971/FRE-NCtools/build3_up_MAXXGRID/tools/make_topog/ if you have access.\n",
    "\n",
    "After this, you need to run expt.bathymetry again, this time passing the flag 'maketopog = False'. It will continue from after the make_topog step and finish the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.bathymetry(\n",
    "    '/g/data/ik11/inputs/GEBCO_2022/GEBCO_2022.nc',\n",
    "    {\"xh\":\"lon\",\n",
    "     \"yh\":\"lat\",\n",
    "     \"elevation\":\"elevation\"}, ## Again this dictionary just maps mom6 variable names to what they are in your topog.\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 (optional) Select number of processors \n",
    "\n",
    "This is just a wrapper for check_mask FRE tool. Choose the number of processors in the X and Y directions respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.processor_mask((10,10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 (optional) Regrid the runoff \n",
    "\n",
    "### This step will be removed in a future update when this functionality is added to rest of pipeline. Currently it calls a function from the legacy regional_model_scripts file. Just execute cell to give your domain runoff from JRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regional_model_scripts import regrid_runoff\n",
    "runoff_path = \"/g/data/ik11/inputs/JRA-55/RYF/v1-3/RYF.runoff_all.1990_1991.nc\" ## Can change to match your year\n",
    "\n",
    "regrid_runoff(inputdir + \"ocean_mask.nc\",\n",
    "    inputdir + \"hgrid.nc\",\n",
    "    runoff_path,\n",
    "    inputdir + \"runoff_regrid.nc\",\n",
    "    np.array(xextent) - np.array([180,180]),\n",
    "    yextent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Modify the default input directory to make a (hopefully) runnable configuration out of the box\n",
    "\n",
    "This cell just copies a default run directory and modifies it to match your configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(f\"cp default_rundir/jra_surface* {rundir}\",shell = True)\n",
    "subprocess.run(f\"ln -s {inputdir} {rundir}/inputdir\",shell=True)\n",
    "\n",
    "hgrid = xr.open_dataset(f\"{inputdir}/hgrid.nc\")\n",
    "\n",
    "## Get mask table information\n",
    "ncpus = 10\n",
    "mask_table = None\n",
    "for i in os.listdir(f\"{inputdir}\"):\n",
    "    if \"mask_table\" in i:\n",
    "        mask_table = i\n",
    "        a = mask_table.split(\".\")[1]\n",
    "        b = mask_table.split(\".\")[2].split(\"x\")\n",
    "        ncpus = int(b[0]) * int(b[1]) - int(a)\n",
    "\n",
    "\n",
    "## Modify MOM_input\n",
    "inputfile = open(f\"{rundir}/MOM_input\",'r')\n",
    "lines = inputfile.readlines()\n",
    "inputfile.close()\n",
    "for i in range(len(lines)):\n",
    "    if \"MASKTABLE\" in lines[i]:\n",
    "        if mask_table != None:\n",
    "            lines[i] = f'MASKTABLE = \"{mask_table}\"\\n'\n",
    "        else:\n",
    "            lines[i] = \"# MASKTABLE = no mask table\"\n",
    "    if \"NIGLOBAL\" in lines[i]: \n",
    "        # lines[i] = f\"NIGLOBAL = {str(x_indices_centre[1] - x_indices_centre[0])}\\n\"\n",
    "        lines[i] = f\"NIGLOBAL = {hgrid.nx.shape[0]//2}\\n\"\n",
    "\n",
    "        \n",
    "    if \"NJGLOBAL\" in lines[i]:\n",
    "        # lines[i] = f\"NJGLOBAL = {str(y_indices_centre[1] - y_indices_centre[0])}\\n\"\n",
    "        lines[i] = f\"NJGLOBAL = {hgrid.ny.shape[0]//2}\\n\"\n",
    "\n",
    "        \n",
    "inputfile = open(f\"{rundir}/MOM_input\",'w')\n",
    "\n",
    "inputfile.writelines(lines)\n",
    "inputfile.close()\n",
    "\n",
    "## Modify SIS_input\n",
    "inputfile = open(f\"{rundir}/SIS_input\",'r')\n",
    "lines = inputfile.readlines()\n",
    "inputfile.close()\n",
    "for i in range(len(lines)):\n",
    "    if \"MASKTABLE\" in lines[i]:\n",
    "        lines[i] = f'MASKTABLE = \"{mask_table}\"\\n'\n",
    "    if \"NIGLOBAL\" in lines[i]:\n",
    "        # lines[i] = f\"NIGLOBAL = {str(x_indices_centre[1] - x_indices_centre[0])}\\n\"\n",
    "        lines[i] = f\"NIGLOBAL = {hgrid.nx.shape[0]//2}\\n\"\n",
    "        \n",
    "    if \"NJGLOBAL\" in lines[i]:\n",
    "        # lines[i] = f\"NJGLOBAL = {str(y_indices_centre[1] - y_indices_centre[0])}\\n\"\n",
    "        lines[i] = f\"NJGLOBAL = {hgrid.ny.shape[0]//2}\\n\"\n",
    "        \n",
    "inputfile = open(f\"{rundir}/SIS_input\",'w')\n",
    "inputfile.writelines(lines)\n",
    "inputfile.close()\n",
    "\n",
    "## Modify config.yaml \n",
    "inputfile = open(f\"{rundir}/config.yaml\",'r')\n",
    "lines = inputfile.readlines()\n",
    "inputfile.close()\n",
    "for i in range(len(lines)):\n",
    "    if \"ncpus\" in lines[i]:\n",
    "        lines[i] = f'ncpus: {str(ncpus)}\\n'\n",
    "    if \"jobname\" in lines[i]:\n",
    "        lines[i] = f\"jobname: mom6_{expt_name}\\n\"\n",
    "        \n",
    "    if \"input:\" in lines[i]:\n",
    "        lines[i + 1] = f\"    - {inputdir}\\n\"\n",
    "        \n",
    "inputfile = open(f\"{rundir}/config.yaml\",'w')\n",
    "inputfile.writelines(lines)\n",
    "inputfile.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS! Want to use ERA5 surface forcing instead?\n",
    "\n",
    "This is WIP and not tested but thought I'd include it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SET UP ERA5 forcing:\n",
    "Here we take the ERA forcing as it already exists on Gadi. For NCI users, you need access to the rt group. ERA5 - specific functions provided cut out the region of interest and fix up the metadata ready for MOM6.\n",
    "\n",
    "For this example, we are forcing for the entire year of 2015 so we just generate a single forcing file with 2015's data.\n",
    "\n",
    "Below is a table showing ERA5 characteristics and what needs to be done to sort it out\n",
    "### Required ERA data:\n",
    "Name | ERA filename | era variable name | notes\n",
    "---|---|---|---\n",
    "Surface Pressure | sp | sp | Pa :heavy_check_mark:\n",
    "Surface Temperature | 2t | t2m | K :heavy_check_mark:\n",
    "Meridional Wind | 10v | v10 | m/s :heavy_check_mark:\n",
    "Zonal Wind | 10u | u10 | m/s :heavy_check_mark:\n",
    "Specific Humidity | na | na | kg/kg, calculated from dewpoint temperature\n",
    "Dewpoint Temperature | 2d | d2m | K\n",
    "\n",
    "\n",
    "We can calculate specific humidity $q$ from dewpoint temperature $T_d$ and surface pressure $P$ via saturation vapour pressure $P_v$.\n",
    "\n",
    "$\\large P_v = 10^{8.07131 - \\frac{1730.63}{233.426 + T}} \\frac{101325}{760} $ Pascals\n",
    "\n",
    "$\\large q = 0.001 * 0.622  \\frac{P_v}{P}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Messy Work in progress for now but works. I think there's an issue with specific humidity units?\n",
    "\n",
    "erapath = \"/g/data/rt52/era5/single-levels/reanalysis\"\n",
    "\n",
    "## Firstly just open all raw data\n",
    "rawdata = {}\n",
    "for fname , vname in zip([\"2t\",\"10u\",\"10v\",\"sp\",\"2d\"] , [\"t2m\",\"u10\",\"v10\",\"sp\",\"d2m\"]):\n",
    "\n",
    "    rawdata[fname] = ml.nicer_slicer(\n",
    "        xr.open_mfdataset(f\"{erapath}/{fname}/{daterange[0].split('-')[0]}/{fname}*\",decode_times = False,chunks = {\"longitude\":100,\"latitude\":100}),\n",
    "        xextent,\n",
    "        \"longitude\"\n",
    "    ).sel(\n",
    "        latitude = slice(yextent[1],yextent[0]) ## This is because ERA5 has latitude in decreasing order (??)\n",
    "    )\n",
    "\n",
    "    ## Now fix up the latitude and time dimensions\n",
    "\n",
    "    rawdata[fname] = rawdata[fname].isel(\n",
    "        latitude = slice(None,None,-1) ## Flip latitude        \n",
    "        ).assign_coords(\n",
    "        time = np.arange(0,rawdata[fname].time.shape[0],dtype=float) ## Set the zero date of forcing to start of run\n",
    "        )\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    rawdata[fname].time.attrs = {\"calendar\":\"julian\",\"units\":f\"hours since {daterange[0]}\"} ## Fix up calendar to match\n",
    "\n",
    "    if fname == \"2d\":\n",
    "        ## Calculate specific humidity from dewpoint temperature \n",
    "        q = xr.Dataset(\n",
    "            data_vars= {\n",
    "                \"q\": 0.001 * (0.622 / rawdata[\"sp\"][\"sp\"]) * (10**(8.07131 - 1730.63 / (233.426 + rawdata[\"2d\"][\"d2m\"]))) * 101325 / 760\n",
    "                }\n",
    "\n",
    "        )\n",
    "        q.q.attrs = {\"long_name\":\"Specific Humidity\",\"units\": \"kg/kg\"}\n",
    "        q.to_netcdf(f\"{inputdir}/forcing/q_ERA5\",unlimited_dims = \"time\",encoding = {\"q\":{\"dtype\":\"double\"}})\n",
    "    else:\n",
    "        rawdata[fname].to_netcdf(f\"{inputdir}/forcing/{fname}_ERA5\",unlimited_dims = \"time\",encoding = {vname:{\"dtype\":\"double\"}})\n",
    "\n",
    "\n",
    "## Update the data table to match:\n",
    "\n",
    "subprocess.run(f\"cp default_rundir/era5_surface/data_table {rundir}/data_table\",shell = True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3]",
   "language": "python",
   "name": "conda-env-analysis3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
