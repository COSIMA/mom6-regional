{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b1c38-d719-4e25-bb73-c970c124f1d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "from itertools import cycle\n",
    "import os\n",
    "import dask\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "from pykdtree.kdtree import KDTree\n",
    "from dask.diagnostics import ProgressBar\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sel_hgrid_indices(field,extent):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "        field    xarray.dataarray   the existing hgrid lon or lat to be cut down to size\n",
    "        extent   list               [min,max] the lat OR lon\n",
    "\n",
    "    Returns:\n",
    "        numpy array containing the start and end indices needed to slice the hgrid/\n",
    "\n",
    "    Function finds the indices corresponding to the start and end of some coordinate range such that the hgrid starts and ends with q points rather than t points. Useful for cutting out hgrid automatically. Note that this doesn't work near the poles in the northern hemisphere.\n",
    "    \n",
    "    It rounds the input field so that 208.99999999 == 209, giving nice even numbers of points between whole number lat/lon bounds\n",
    "    \n",
    "    This function is lazily implemented. Handling the edge cases was easiest to do without vectorising, but there should be numpy functions that would make this less inefficient.\n",
    "    \"\"\"\n",
    "    temp = False\n",
    "\n",
    "    indices = []\n",
    "\n",
    "    for i in range(field.shape[0]):\n",
    "        if round(field[i].values.reshape(1)[0],6) >= extent[0] and temp == False:\n",
    "            indices.append(2 * i)\n",
    "            temp = True\n",
    "        elif round(field[i].values.reshape(1)[0],6) > extent[1] and temp == True:\n",
    "            indices.append((i-1) * 2 + 1) ## This goes back a step to find the last t point that was in the bounds. Then goes forward one so that the final point to slice is on the corner\n",
    "            temp = False\n",
    "            break\n",
    "    \n",
    "    return np.array(indices)\n",
    "\n",
    "om2path = \"/g/data/ik11/inputs/access-om2/input_08022019/mom_01deg/\" ## Access om2_01 input for topography and hgrid\n",
    "initpath = \"/g/data/ik11/outputs/access-om2-01/01deg_jra55v13_ryf9091/output1077\" ## Access om2_01 output for BCs and IC\n",
    "toolpath = \"/home/157/ahg157/repos/mom5/src/tools/\" ## Compiled tools needed for construction of mask tables\n",
    "    \n",
    "xr.set_options(keep_attrs=True) ## This ensures that performing simple operations with xarray (eg converting temperature to Celsius) preserves attributes\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd34e0b-c443-4d11-8c64-2557d96d7b74",
   "metadata": {},
   "source": [
    "# What this notebook does:\n",
    "\n",
    "Given a choice of lat / lon coordinates, a hgrid is built from an existing access om2_01 global run. Then, all of the other required input files are interpolated onto this hgrid, and Angus Gibson's 'Brushcutter' creates files for boundary files. It also sets up the mask tables and regrids the runoff, sorting out everything you need to run the experiment.\n",
    "\n",
    "An input directory is copied into the 'rundir' you choose, and a couple of files are automatically modified based on your experiment. As longs as you don't change your default layout, you\n",
    "\n",
    "# What's still to do:\n",
    "\n",
    "* Test whether this setup works for a pan-antarctic style domain. In that case you'd need to set REENTRANT_X = True in MOM_input. Otherwise there might be some other things that need changing to ensure that the input files wrap around properly\n",
    "* Automate the procedure for changing the resolution. I think this can be done by constructing a new hgrid of whatever resolution, then interpolating everything else onto it as before. Would require reading in a high resolution topography, but otherwise could use the access om2-01 forcing at boundary and initially\n",
    "* Handle the case where you want to look at a domain that straddles the 'seam' in longitude coordinate at -180.\n",
    "\n",
    "# Troubleshooting:\n",
    "* You can get some really weird errors if there are little bays trapped on your boundaries. Try to either avoid this, or fill them in manually if they appear. This is what I was getting: \n",
    "\n",
    "\"FATAL from PE    56: MOM_regridding: adjust_interface_motion() - implied h<0 is larger than roundoff!\"\n",
    "\n",
    "* If you alter the remapping code, be sure to check that your netcdf files have sensible attributes. You can check yours against a working model by ensuring your attributes match with /scratch/v45/ab8992/mom6/regional_configs/ttide-new/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f4c236-e55d-4d00-9207-0353772bb814",
   "metadata": {},
   "source": [
    "## Choose your Domain, and set path to your input and run directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b814735-26c1-4bae-b734-06cc79e71753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Choose your coordinates and the name of your experiment\n",
    "yextent = [-55,-20]\n",
    "xextent = [-217 , -210]\n",
    "expt_name = \"test\"\n",
    "\n",
    "## Place where all your input files go\n",
    "path = f\"/scratch/v45/ab8992/mom6/regional_configs/{expt_name}/\"\n",
    "\n",
    "## Directory where you'll run the experiment from\n",
    "rundir = f\"/home/149/ab8992/mom6_rundirs/{expt_name}\"\n",
    "\n",
    "for i in [path,rundir]:\n",
    "    try:\n",
    "        os.mkdir(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if \"temp\" not in os.listdir(path):\n",
    "    os.mkdir(path + \"temp\")\n",
    "if \"weights\" not in os.listdir(path):\n",
    "    os.mkdir(path + \"weights\")\n",
    "if \"forcing\" not in os.listdir(path):\n",
    "    os.mkdir(path + \"forcing\")\n",
    "\n",
    "\n",
    "full_domain = xr.open_dataset(f\"{initpath}/ocean/ocean_daily.nc\")\n",
    "\n",
    "# domain.tau_x.sel(yu_ocean=slice(-49, -5), xu_ocean=slice(-217, -183)).isel(time = 0).plot()\n",
    "domain = full_domain.surface_temp.sel(yt_ocean=slice(yextent[0], yextent[1]), xt_ocean=slice(xextent[0],xextent[1])).isel(time = 0)\n",
    "fig,ax = plt.subplots(1,figsize = (10,10))\n",
    "\n",
    "domain.plot(ax = ax)\n",
    "ax.set_title(f\"{expt_name} domain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b651846a-d772-47d7-8808-bda27b7eb3b3",
   "metadata": {},
   "source": [
    "# Step by step procedure to set up experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39149e88-c9ff-421e-ba16-824e502da083",
   "metadata": {},
   "source": [
    "## 1. Find the indices of the hgrid that we need based on the lat/lon chosen above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b9d1c-d917-453a-b8a1-f269c5738cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hgrid = xr.open_dataset(om2path + \"ocean_hgrid.nc\")\n",
    "\n",
    "x_indices_hgrid = sel_hgrid_indices(hgrid.x.isel(nyp = 1000,nxp = slice(0,None,2)),xextent)  ## As long as it's away from the north pole things are ok since lon doesn't vary with lat\n",
    "y_indices_hgrid = sel_hgrid_indices(hgrid.y.isel(nxp = 0,nyp = slice(0,None,2)),yextent) \n",
    "\n",
    "encoding = {'x': {'_FillValue': None},\n",
    "            'y': {'_FillValue': None},\n",
    "            \"dx\": {\"_FillValue\": None},\n",
    "            'dy': {'_FillValue': None},\n",
    "            'angle_dx': {'_FillValue': None},\n",
    "            'area': {'_FillValue': None}\n",
    "            }  \n",
    "\n",
    "hgrid_new = hgrid.isel(\n",
    "    nyp = slice(y_indices_hgrid[0] , y_indices_hgrid[1] ),\n",
    "    nxp = slice(x_indices_hgrid[0] , x_indices_hgrid[1] ),\n",
    "    ny = slice(y_indices_hgrid[0] , y_indices_hgrid[1] - 1 ),\n",
    "    nx = slice(x_indices_hgrid[0] , x_indices_hgrid[1] - 1 ))\n",
    "\n",
    "hgrid_new.to_netcdf(path + \"hgrid.nc\",mode = \"w\",encoding = encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac9921f-ed87-4fca-ac20-298e5325f34b",
   "metadata": {},
   "source": [
    "## 2. Cut out the initial condition velocities and interpolate onto the cgrid\n",
    "\n",
    "Here we can use the normal xarray select tool to get the initial velocities. This is set by default to be at time=30, as this is how Angus set things up. \n",
    "\n",
    "The horizontal coordinates are a bit of a mess. Initially, the mom5 data uses xu_ocean,xt_ocean etc. In MOM6, yh,xq etc are used. Here, they're renamed to \"lat/lon\" for the regridder. Afterwards, the velocities are named in the \"xh,yq\" convention, but the tracers aren't. This is to be consistent with Angus' example EAC configuration. I suspect there would be no issue renaming the coordinates consistently if the relevent MOM files were altered to match. \n",
    "\n",
    "Note also that the \"encoding\" parameters are needed to overwrite xarray's default values. xr will put in its own NaNs as the _FillVallue which aren't compatible with MOM6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b81a05-785a-4276-860e-36e6126edff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## pull out the initial velocity on MOM5's Bgrid\n",
    "u_init_bgrid = xr.open_dataset(f\"{initpath}/ocean/ocean_daily_3d_u.nc\").u.rename({\"xu_ocean\": \"lon\", \"yu_ocean\": \"lat\"}).sel(\n",
    "    lon = slice(xextent[0],xextent[1]),lat = slice(yextent[0],yextent[1])).isel(time = 30)\n",
    "v_init_bgrid = xr.open_dataset(f\"{initpath}/ocean/ocean_daily_3d_v.nc\").v.rename({\"xu_ocean\": \"lon\", \"yu_ocean\": \"lat\"}).sel(\n",
    "    lon = slice(xextent[0],xextent[1]),lat = slice(yextent[0],yextent[1])).isel(time = 30)\n",
    "\n",
    "## Pull out the surface height, temp and salinity initial conds\n",
    "temperature = xr.open_dataset(f\"{initpath}/ocean/ocean_daily_3d_temp.nc\").temp.isel(time = 30).sel(\n",
    "    yt_ocean = slice(yextent[0],yextent[1]),xt_ocean = slice(xextent[0],xextent[1]))  - 273.15## Convert to Celcius for MOM6!\n",
    "salt = xr.open_dataset(f\"{initpath}/ocean/ocean_daily_3d_salt.nc\").salt.isel(time = 30).sel(\n",
    "    yt_ocean = slice(yextent[0],yextent[1]),xt_ocean = slice(xextent[0],xextent[1]))\n",
    "\n",
    "## Merge tracer ICs together\n",
    "init_cond = xr.merge([temperature,salt]).drop_vars(\"time\").rename({\"xt_ocean\": \"lon\", \"yt_ocean\": \"lat\"})\n",
    "\n",
    "init_eta = xr.open_dataset(f\"{initpath}/ocean/ocean_daily.nc\").isel(time = 30).sel(\n",
    "    yt_ocean = slice(yextent[0],yextent[1]),xt_ocean = slice(xextent[0],xextent[1])).rename({\"xt_ocean\": \"lon\", \"yt_ocean\": \"lat\"})\n",
    "\n",
    "\n",
    "## Construct the xq,yh and xh yq grids\n",
    "ugrid = hgrid_new[[\"x\",\"y\"]].isel(nxp=slice(None, None, 2), nyp=slice(1, None, 2)).rename({\"x\": \"lon\", \"y\": \"lat\"})\n",
    "vgrid = hgrid_new[[\"x\",\"y\"]].isel(nxp=slice(1, None, 2), nyp=slice(None, None, 2)).rename({\"x\": \"lon\", \"y\": \"lat\"})\n",
    "\n",
    "## Construct the cell centre grid for tracers (xh,yh). \n",
    "tgrid = xr.Dataset(\n",
    "    {\"lon\":([\"lon\"],hgrid_new.x.isel(nxp=slice(1, None, 2), nyp=1).values),\n",
    "     \"lat\":([\"lat\"],hgrid_new.y.isel(nxp=1, nyp=slice(1, None, 2)).values)\n",
    "            }\n",
    ")\n",
    "\n",
    "\n",
    "regridder_u = xe.Regridder(\n",
    "    u_init_bgrid, ugrid, \"bilinear\",\n",
    ")\n",
    "regridder_v = xe.Regridder(\n",
    "    v_init_bgrid, vgrid, \"bilinear\",\n",
    ")\n",
    "\n",
    "regridder_t = xe.Regridder(\n",
    "    init_cond, tgrid, \"bilinear\",\n",
    ")\n",
    "\n",
    "u_on_c = regridder_u(u_init_bgrid)\n",
    "\n",
    "print(u_on_c.coords)\n",
    "\n",
    "u_on_c = u_on_c.rename({\"lon\": \"xq\", \"lat\": \"yh\", \"nyp\": \"ny\"})\n",
    "print(u_on_c.coords)\n",
    "\n",
    "u_on_c.name = \"u\"\n",
    "## Give attributes back\n",
    "u_on_c.attrs = u_init_bgrid.attrs \n",
    "u_on_c.xq.attrs = u_init_bgrid.lon.attrs\n",
    "u_on_c.yh.attrs = u_init_bgrid.lat.attrs\n",
    "u_on_c.st_ocean.attrs = u_init_bgrid.st_ocean.attrs\n",
    "\n",
    "v_on_c = regridder_v(v_init_bgrid)\n",
    "v_on_c = v_on_c.rename({\"lon\": \"xh\", \"lat\": \"yq\", \"nxp\": \"nx\"})\n",
    "v_on_c.name = \"v\"\n",
    "## Give attributes back\n",
    "v_on_c.attrs = v_init_bgrid.attrs \n",
    "v_on_c.xh.attrs = v_init_bgrid.lon.attrs\n",
    "v_on_c.yq.attrs = v_init_bgrid.lat.attrs\n",
    "v_on_c.st_ocean.attrs = v_init_bgrid.st_ocean.attrs\n",
    "\n",
    "vel_init_cgrid = xr.merge((u_on_c, v_on_c),)\n",
    "vel_init_cgrid.st_ocean.attrs[\"axis\"] = \"Z\"\n",
    "\n",
    "vel_init_cgrid.fillna(0).drop_vars(\"time\").to_netcdf(\n",
    "    path + \"forcing/init_vel_cgrid.nc\",\n",
    "    mode = \"w\",\n",
    "    encoding={\n",
    "        \"u\": {\"_FillValue\": netCDF4.default_fillvals[\"f4\"]},\n",
    "        \"v\": {\"_FillValue\": netCDF4.default_fillvals[\"f4\"]}        \n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "## regrid tracers\n",
    "init_cond_rg = regridder_t(init_cond)\n",
    "init_cond_rg = init_cond_rg.rename({\"lon\": \"xt_ocean\", \"lat\": \"yt_ocean\"})\n",
    "## Give attributes back\n",
    "init_cond_rg.attrs = init_cond.attrs \n",
    "init_cond_rg.xt_ocean.attrs = init_cond.lon.attrs\n",
    "init_cond_rg.yt_ocean.attrs = init_cond.lat.attrs\n",
    "init_cond_rg.st_ocean.attrs = init_cond.st_ocean.attrs\n",
    "\n",
    "init_cond_rg.to_netcdf(\n",
    "    path + \"forcing/init_cond.nc\",\n",
    "    mode = \"w\",\n",
    "    encoding = {'xt_ocean': {'_FillValue': None},\n",
    "                'yt_ocean': {'_FillValue': None},\n",
    "                \"st_ocean\": {\"_FillValue\": None},\n",
    "                'temp': {'_FillValue': -1e20,'missing_value': -1e20},\n",
    "                'salt': {'_FillValue': -1e20,'missing_value': -1e20}\n",
    "                },\n",
    ")\n",
    "\n",
    "\n",
    "## regrid eta\n",
    "init_eta_rg = regridder_t(init_eta.eta_t)\n",
    "init_eta_rg = init_eta_rg.rename({\"lon\": \"xt_ocean\", \"lat\": \"yt_ocean\"})\n",
    "\n",
    "## Give attributes back\n",
    "init_eta_rg.attrs = init_eta.attrs \n",
    "init_eta_rg.xt_ocean.attrs = init_eta.lon.attrs\n",
    "init_eta_rg.yt_ocean.attrs = init_eta.lat.attrs\n",
    "\n",
    "init_eta_rg.name = \"eta_t\"\n",
    "\n",
    "init_eta_rg.drop_vars(\"time\").to_netcdf(\n",
    "    path + \"forcing/init_eta.nc\",\n",
    "    mode = \"w\",\n",
    "    encoding = {'xt_ocean': {'_FillValue': None},\n",
    "                'yt_ocean': {'_FillValue': None},\n",
    "                'eta_t':{'_FillValue':None}\n",
    "                },\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aca09c-7ba0-497e-90a6-8a744a62b322",
   "metadata": {},
   "source": [
    "## 3 Get the topography, and cut out using the indices from hgrid\n",
    "\n",
    "For now, this step assumes that we're just cutting the hgrid and topo from the same mom5 experiment. This means that the indices for cutting out the hgrid can be used to cut out the topo and runoff. If resolution changes, hgrid and topo will generated in some other way. Could also interpolate a topography file the same way as initial conditions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fcc4cd-8491-42f2-b52a-7435d76ba847",
   "metadata": {},
   "outputs": [],
   "source": [
    "topog = xr.open_dataset(\"/g/data/ik11/inputs/access-om2/input_08022019/mom_01deg/topog.nc\").rename({\"xx\":\"nx\",\"yy\":\"ny\"})\n",
    "topog = topog.expand_dims({'ntiles':1})\n",
    "\n",
    "x_indices_centre = x_indices_hgrid //2    ## To go from the hgrid's indices (which refer to cell edges) to cell centres, we just need to //2. \n",
    "y_indices_centre = y_indices_hgrid //2    ## For cell corners, we'd need to go hgrid //2 for start of slice, //2 + 1 for end of slice\n",
    "\n",
    "encoding = {\"depth\":{'_FillValue': None}} \n",
    "\n",
    "\n",
    "topog.isel(nx = slice(x_indices_centre[0],x_indices_centre[1]) , ny = slice(y_indices_centre[0],y_indices_centre[1])).to_netcdf(path + \"topog.nc\",mode = \"w\",encoding = encoding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74918e1-2492-4672-9f57-d001f753e12f",
   "metadata": {},
   "source": [
    "## 4. Use the c executables from mom5 tools to construct the mosaic and mask tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf62eb-2bef-4336-9216-2d592fa0dbd4",
   "metadata": {},
   "source": [
    "# Make ocean mosaic\n",
    "\n",
    "This requires running some \"mom5 tools\" scripts via subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4448cbc0-e916-4f6a-844e-35a1756cc866",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = \"--num_tiles 1 --dir . --mosaic_name ocean_mosaic --tile_file hgrid.nc\".split(\" \")\n",
    "\n",
    "print(\"MAKE SOLO MOSAIC\",subprocess.run([toolpath + \"make_solo_mosaic/make_solo_mosaic\"] + args,cwd = path),sep = \"\\n\")\n",
    "\n",
    "args = \"--input_mosaic ocean_mosaic.nc --mosaic_name grid_spec --ocean_topog topog.nc\".split(\" \")\n",
    "\n",
    "print(\"QUICK MOSAIC\" , subprocess.run([toolpath + \"make_quick_mosaic/make_quick_mosaic\"] + args,cwd = path),sep = \"\\n\")\n",
    "\n",
    "args = \"--grid_file ocean_mosaic.nc --ocean_topog topog.nc --layout 10,10 --halo 4\".split(\" \")\n",
    "\n",
    "print(\"CHECK MASK\" , subprocess.run([toolpath + \"check_mask/check_mask\"] + args,cwd = path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4292a4ba-62af-4ed0-ac97-d8162d4c48e4",
   "metadata": {},
   "source": [
    "# 5. Prepare boundary segments\n",
    "\n",
    "Use modified code from Angus' scripts. This cuts out the relevant boundary fields for the experiment and saves them to memory on the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d70435-655f-40f9-b181-e2537d4fba54",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = range(1077, 1082) # original\n",
    "# t = range(1077, 1079)\n",
    "base = os.getenv(\"PBS_JOBFS\")\n",
    "\n",
    "surface_tracer_vars = [\"temp\", \"salt\"]\n",
    "line_tracer_vars = [\"eta_t\"]\n",
    "surface_velocity_vars = [\"u\", \"v\"]\n",
    "surface_vars = surface_tracer_vars + surface_velocity_vars\n",
    "\n",
    "chunks = {\n",
    "    \"T\": {\"time\": 1, \"st_ocean\": 7, \"yt_ocean\": 300, \"xt_ocean\": None},\n",
    "    \"U\": {\"time\": 1, \"st_ocean\": 7, \"yu_ocean\": 300, \"xu_ocean\": None},\n",
    "}\n",
    "\n",
    "def time_rotate(d):\n",
    "    left = d.sel(time=slice(\"2171-01-01\", None))\n",
    "    left[\"time\"] = pd.date_range(\"1991-01-01 12:00:00\", periods=120)\n",
    "\n",
    "    right = d.sel(time=slice(None, \"2170-12-31\"))\n",
    "    right[\"time\"] = pd.date_range(\"1991-05-01 12:00:00\", periods=245)\n",
    "\n",
    "    return xr.concat([left, right], \"time\")\n",
    "\n",
    "in_datasets = {}\n",
    "for var, staggering in list(zip(surface_tracer_vars, cycle(\"T\"))) + list(\n",
    "    zip(surface_velocity_vars, cycle(\"U\"))\n",
    "):\n",
    "    d = xr.open_mfdataset(\n",
    "        [\n",
    "            f\"/g/data/ik11/outputs/access-om2-01/01deg_jra55v13_ryf9091/output{i}/ocean/ocean_daily_3d_{var}.nc\"\n",
    "            for i in t\n",
    "        ],\n",
    "        chunks=chunks[staggering],\n",
    "        combine=\"by_coords\",\n",
    "        parallel=False,\n",
    "    )[var]\n",
    "    in_datasets[var] = staggering, d\n",
    "\n",
    "# line datasets, assume they all come from ocean_daily\n",
    "d_2d = xr.open_mfdataset(\n",
    "    [\n",
    "        f\"/g/data/ik11/outputs/access-om2-01/01deg_jra55v13_ryf9091/output{i}/ocean/ocean_daily.nc\"\n",
    "        for i in t\n",
    "    ],\n",
    "    chunks={\"time\": 1, \"yt_ocean\": 300, \"xt_ocean\": None},\n",
    "    combine=\"by_coords\",\n",
    "    parallel=False,\n",
    ")[line_tracer_vars]\n",
    "\n",
    "d_tracer = xr.merge([d for s, d in in_datasets.values() if s == \"T\"] + [d_2d])\n",
    "d_velocity = xr.merge([d for s, d in in_datasets.values() if s == \"U\"])\n",
    "\n",
    "# time slicing\n",
    "\n",
    "d_tracer = time_rotate(d_tracer.sel(time=slice(\"2170-05-01\", \"2171-04-30\")))\n",
    "d_velocity = time_rotate(d_velocity.sel(time=slice(\"2170-05-01\", \"2171-04-30\")))\n",
    "\n",
    "# reduce selection around target latitude\n",
    "# and remove spatial chunks (required for xesmf)\n",
    "d_tracer = d_tracer.sel(yt_ocean=slice(yextent[0] - 1, yextent[1] + 1), xt_ocean=slice(xextent[0] - 1,xextent[1] + 1)).chunk(\n",
    "    {\"yt_ocean\": None, \"xt_ocean\": None}\n",
    ")\n",
    "d_velocity = d_velocity.sel(yu_ocean=slice(yextent[0] - 1, yextent[1] + 1), xu_ocean=slice(xextent[0] - 1,xextent[1] + 1)).chunk(\n",
    "    {\"yu_ocean\": None, \"xu_ocean\": None}\n",
    ")\n",
    "\n",
    "with ProgressBar():\n",
    "    d_tracer.to_zarr(\n",
    "        f\"{base}/tracer.zarr\",\n",
    "        encoding={\"time\": {\"dtype\": \"double\", \"units\": \"days since 1900-01-01 12:00:00\", \"calendar\": \"noleap\"}},\n",
    "        mode = \"w\"\n",
    "    )\n",
    "\n",
    "with ProgressBar():\n",
    "    d_velocity.to_zarr(\n",
    "        f\"{base}/velocity.zarr\",\n",
    "        encoding={\"time\": {\"dtype\": \"double\", \"units\": \"days since 1900-01-01 12:00:00\", \"calendar\": \"noleap\"}},\n",
    "        mode = \"w\"\n",
    "    )\n",
    "    \n",
    "## If you get some error about path containing a group it's because the zarr files already exist. Delete them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119262c2-e463-42c7-84a5-40887b5c177f",
   "metadata": {},
   "source": [
    "# 6. Brushcut\n",
    "\n",
    "Use Angus' brushcut script to generate the boundary forcing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1513e07-606b-4a61-9242-ad1223ff60fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surface_tracer_vars = [\"temp\", \"salt\"]\n",
    "line_tracer_vars = [\"eta_t\"]\n",
    "surface_velocity_vars = [\"u\", \"v\"]\n",
    "surface_vars = surface_tracer_vars + surface_velocity_vars\n",
    "\n",
    "\n",
    "\n",
    "def input_datasets():\n",
    "    # open target grid dataset\n",
    "    # we interpolate onto the hgrid\n",
    "    dg = xr.open_dataset(path + \"hgrid.nc\")\n",
    "\n",
    "    d_tracer = xr.open_zarr(f\"{os.getenv('PBS_JOBFS')}/tracer.zarr\")\n",
    "    d_velocity = xr.open_zarr(f\"{os.getenv('PBS_JOBFS')}/velocity.zarr\")\n",
    "\n",
    "    return dg, d_tracer, d_velocity\n",
    "\n",
    "def interp_segment(segment):\n",
    "    dg, d_tracer, d_velocity = input_datasets()\n",
    "    i, edge = segment\n",
    "\n",
    "    dg_segment = dg.isel(**edge)\n",
    "    # interpolation grid\n",
    "    dg_out = xr.Dataset(\n",
    "        {\n",
    "            \"lat\": ([\"location\"], dg_segment.y.squeeze().data),\n",
    "            \"lon\": ([\"location\"], dg_segment.x.squeeze().data),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # segment suffix\n",
    "    seg = f\"segment_{i+1:03}\"\n",
    "    seg_dir = [\"nx\", \"nx\", \"ny\", \"ny\"][i]\n",
    "    seg_alt = [\"ny\", \"ny\", \"nx\", \"nx\"][i]\n",
    "    alt_axis = [2, 2, 3, 3][i]\n",
    "\n",
    "    # create the regridding weights between our grids\n",
    "    # note: reuse_weights should be False unless the weights files\n",
    "    #       do indeed exist!\n",
    "    regridder_tracer = xe.Regridder(\n",
    "        d_tracer.rename(xt_ocean=\"lon\", yt_ocean=\"lat\"),\n",
    "        dg_out,\n",
    "        \"bilinear\",\n",
    "        locstream_out=True,\n",
    "        reuse_weights=False,\n",
    "        filename= path + f\"weights/bilinear_tracer_weights_{seg}.nc\",\n",
    "    )\n",
    "    regridder_velocity = xe.Regridder(\n",
    "        d_velocity.rename(xu_ocean=\"lon\", yu_ocean=\"lat\"),\n",
    "        dg_out,\n",
    "        \"bilinear\",\n",
    "        locstream_out=True,\n",
    "        reuse_weights=False,\n",
    "        filename=path + f\"weights/bilinear_velocity_weights_{seg}.nc\",\n",
    "    )\n",
    "\n",
    "    # now we can apply it to input DataArrays:\n",
    "    segment_out = xr.merge([regridder_tracer(d_tracer), regridder_velocity(d_velocity)])\n",
    "    del segment_out[\"lon\"]\n",
    "    del segment_out[\"lat\"]\n",
    "    segment_out[\"temp\"] -= 273.15\n",
    "\n",
    "    # fill in NaNs\n",
    "    segment_out = (\n",
    "        segment_out\n",
    "        .ffill(\"st_ocean\")\n",
    "        .interpolate_na(\"location\")\n",
    "        .ffill(\"location\")\n",
    "        .bfill(\"location\")\n",
    "    )\n",
    "\n",
    "    # fix up all the coordinate metadata\n",
    "    segment_out = segment_out.rename(location=f\"{seg_dir}_{seg}\")\n",
    "    for var in surface_vars:\n",
    "        segment_out[var] = segment_out[var].rename(st_ocean=f\"nz_{seg}_{var}\")\n",
    "        segment_out = segment_out.rename({var: f\"{var}_{seg}\"})\n",
    "        segment_out[f\"nz_{seg}_{var}\"] = np.arange(segment_out[f\"nz_{seg}_{var}\"].size)\n",
    "\n",
    "    for var in line_tracer_vars:\n",
    "        segment_out = segment_out.rename({var: f\"{var}_{seg}\"})\n",
    "\n",
    "    # segment coordinates (x, y, z)\n",
    "    segment_out[f\"{seg_dir}_{seg}\"] = np.arange(segment_out[f\"{seg_dir}_{seg}\"].size)\n",
    "    segment_out[f\"{seg_alt}_{seg}\"] = [0]\n",
    "\n",
    "    # lat/lon/depth/dz\n",
    "    segment_out[f\"lon_{seg}\"] = ([f\"ny_{seg}\", f\"nx_{seg}\"], dg_segment.x.data)\n",
    "    segment_out[f\"lat_{seg}\"] = ([f\"ny_{seg}\", f\"nx_{seg}\"], dg_segment.y.data)\n",
    "\n",
    "    # reset st_ocean so it's not an index coordinate\n",
    "    segment_out = segment_out.reset_index(\"st_ocean\").reset_coords(\"st_ocean_\")\n",
    "    depth = segment_out[\"st_ocean_\"]\n",
    "    depth.name = \"depth\"\n",
    "    depth[\"st_ocean\"] = np.arange(depth[\"st_ocean\"].size)\n",
    "    del segment_out[\"st_ocean_\"]\n",
    "\n",
    "    # some fiddling to do dz in the same way as brushcutter, while making xarray happy\n",
    "    dz = depth.diff(\"st_ocean\")\n",
    "    dz.name = \"dz\"\n",
    "    dz = xr.concat([dz, dz[-1]], dim=\"st_ocean\")\n",
    "    dz[\"st_ocean\"] = depth[\"st_ocean\"]\n",
    "\n",
    "    encoding_dict = {\n",
    "        \"time\": {\n",
    "            \"dtype\": \"double\",\n",
    "            \"units\": \"days since 1900-01-01 12:00:00\",\n",
    "            \"calendar\": \"noleap\",\n",
    "        },\n",
    "        f\"nx_{seg}\": {\n",
    "            \"dtype\": \"int32\",\n",
    "        },\n",
    "        f\"ny_{seg}\": {\n",
    "            \"dtype\": \"int32\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for var in line_tracer_vars:\n",
    "        v = f\"{var}_{seg}\"\n",
    "\n",
    "        segment_out[v] = segment_out[v].expand_dims(\n",
    "            f\"{seg_alt}_{seg}\", axis=alt_axis - 1\n",
    "        )\n",
    "\n",
    "        encoding_dict[v] = {\n",
    "            \"_FillValue\": netCDF4.default_fillvals[\"f8\"],\n",
    "        }\n",
    "\n",
    "    for var in surface_vars:\n",
    "        v = f\"{var}_{seg}\"\n",
    "\n",
    "        # add the y dimension\n",
    "        segment_out[v] = segment_out[v].expand_dims(\n",
    "            f\"{seg_alt}_{seg}\", axis=alt_axis\n",
    "        )\n",
    "        segment_out[f\"dz_{v}\"] = (\n",
    "            [\"time\", f\"nz_{v}\", f\"ny_{seg}\", f\"nx_{seg}\"],\n",
    "            da.broadcast_to(\n",
    "                dz.data[None, :, None, None],\n",
    "                segment_out[v].shape,\n",
    "                chunks=(1, None, None, None),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        s = list(segment_out[v].shape)\n",
    "        s[0] = 1 # chunked in time\n",
    "        s[1] = 11 # a little bit of vertical chunking\n",
    "\n",
    "        encoding_dict[v] = {\n",
    "            \"_FillValue\": netCDF4.default_fillvals[\"f8\"],\n",
    "            \"zlib\": True,\n",
    "            \"chunksizes\": tuple(s),\n",
    "        }\n",
    "        encoding_dict[f\"dz_{v}\"] = {\n",
    "            \"_FillValue\": netCDF4.default_fillvals[\"f8\"],\n",
    "            \"zlib\": True,\n",
    "            \"chunksizes\": tuple(s),\n",
    "        }\n",
    "        encoding_dict[f\"nz_{seg}_{var}\"] = {\n",
    "            \"dtype\": \"int32\"\n",
    "        }\n",
    "\n",
    "    with ProgressBar():\n",
    "        segment_out[\"time\"] = segment_out[\"time\"].assign_attrs({\"modulo\":\" \"}) ## Add modulo attribute for MOM6 to treat as repeat forcing\n",
    "        segment_out.load().to_netcdf(path + f\"forcing/forcing_obc_{seg}.nc\", encoding=encoding_dict, unlimited_dims=\"time\")\n",
    "\n",
    "\n",
    "\n",
    "for seg in enumerate([{\"nyp\": [0]}, {\"nyp\": [-1]}, {\"nxp\": [0]}, {\"nxp\": [-1]}]):\n",
    "    interp_segment(seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77766d70-e946-46cc-ae72-034c5a454f35",
   "metadata": {},
   "source": [
    "# 7. Cut Out and Regrid Runoff\n",
    "\n",
    "Pull out runoff from JRA input, then run Angus' regrid runoff script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b854dac-df66-4717-8359-f231824cad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cut runoff from JRA input\n",
    "\n",
    "JRA = \"/g/data/ik11/inputs/JRA-55/RYF/v1-3/RYF.runoff_all.1990_1991.nc\"\n",
    "\n",
    "jra_runoff = xr.open_dataset(JRA).sel(latitude = slice(yextent[0],yextent[1]),\n",
    "         longitude = slice(xextent[0] + 360,xextent[1] + 360) ## need to add 360 since xextent is between -280 -> 80\n",
    "        ) \n",
    "\n",
    "# ocean mask and supergrid (reduced to tracer points) for the target grid\n",
    "dm = xr.open_dataset(path + \"ocean_mask.nc\").rename({\"nx\": \"longitude\", \"ny\": \"latitude\"})\n",
    "dg = (\n",
    "    xr.open_dataset(path + \"hgrid.nc\")\n",
    "    .isel(nxp=slice(1, None, 2), nyp=slice(1, None, 2))\n",
    "    .rename({\"nyp\": \"latitude\", \"nxp\": \"longitude\"})\n",
    ")\n",
    "\n",
    "# merge areas to get full cell area\n",
    "area = dg.area\n",
    "area[\"ny\"] = area.ny // 2\n",
    "area[\"nx\"] = area.nx // 2\n",
    "area = (\n",
    "    area\n",
    "    .stack(cell=[\"ny\", \"nx\"])\n",
    "    .groupby(\"cell\")\n",
    "    .sum()\n",
    "    .unstack(\"cell\")\n",
    ")\n",
    "\n",
    "# calculate coastal mask\n",
    "cst = xr.zeros_like(dm.mask)\n",
    "for dim in [\"longitude\", \"latitude\"]:\n",
    "    for off in [-1, 1]:\n",
    "        cst = xr.where((dm.mask > 0) & (dm.mask.shift(**{dim: off}) == 0), 1, cst)\n",
    "\n",
    "# indices of coast points -- Nx2, first column are y indices, then x indices\n",
    "cst_pts = np.vstack(np.nonzero(cst)).T\n",
    "# coords of coast points -- Nx2, first column are latitudes, then longitudes\n",
    "cst_coords = xr.concat((dg.y, dg.x + 360), \"d\").data.reshape(2, -1).T[np.flatnonzero(cst)]\n",
    "cst_areas = area.data.flatten()[np.flatnonzero(cst)]\n",
    "\n",
    "kd = KDTree(cst_coords)\n",
    "\n",
    "# open the runoff section and construct its corner points\n",
    "# dr = xr.open_dataset(\"runoff_box.nc\")\n",
    "dr = jra_runoff\n",
    "res = 0.25\n",
    "lons = np.arange(dr.longitude[0] - res/2, dr.longitude[-1] + res, res)\n",
    "lats = np.arange(dr.latitude[0] - res/2,  dr.latitude[-1] + res, res)\n",
    "\n",
    "# source coords for remapping\n",
    "runoff_coords = np.c_[np.meshgrid(dr.latitude, dr.longitude, indexing=\"ij\")].reshape(2, -1).T\n",
    "# coords for cell area calculation\n",
    "corner_lat, corner_lon = np.meshgrid(np.deg2rad(lats), np.deg2rad(lons), indexing=\"ij\")\n",
    "Re = 6378.137e3\n",
    "runoff_areas = np.abs(\n",
    "    ((corner_lon[1:,1:] - corner_lon[:-1,:-1]) * Re**2) * (np.sin(corner_lat[1:,1:]) - np.sin(corner_lat[:-1,:-1]))\n",
    ")\n",
    "\n",
    "# nearest coastal point for every runoff point\n",
    "_, nearest_cst = kd.query(runoff_coords)\n",
    "\n",
    "# create output DataArray\n",
    "runoff = xr.DataArray(\n",
    "    0.0,\n",
    "    {\"time\": dr.time, \"latitude\": dg.y.isel(longitude=0), \"longitude\": dg.x.isel(latitude=0)},\n",
    "    [\"time\", \"latitude\", \"longitude\"]\n",
    ")\n",
    "runoff.name = \"friver\"\n",
    "runoff.time.attrs[\"modulo\"] = \" \"\n",
    "\n",
    "ind_y = xr.DataArray(cst_pts[:,0], dims=\"coast\")\n",
    "ind_x = xr.DataArray(cst_pts[:,1], dims=\"coast\")\n",
    "\n",
    "for i in range(dr.time.size):\n",
    "    # list of nearest coast point (on target grid), with the source data\n",
    "    dat = np.c_[nearest_cst, (dr.friver[i].data * runoff_areas).flatten()]\n",
    "    dat = dat[dat[:,0].argsort()] # sort by coast point idx\n",
    "\n",
    "    # group by destination point\n",
    "    cst_point, split_idx = np.unique(dat[:,0], return_index=True)\n",
    "    cst_point = cst_point.astype(int)\n",
    "    split_idx = split_idx[1:]\n",
    "\n",
    "    # sum per destination point\n",
    "    dat_cst = [x.sum() for x in np.split(dat[:,1], split_idx)]\n",
    "\n",
    "    # assign the target value\n",
    "    runoff[i, ind_y[cst_point], ind_x[cst_point]] = dat_cst / cst_areas[cst_point]\n",
    "\n",
    "runoff.to_netcdf(path + \"runoff_regrid.nc\", unlimited_dims=\"time\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a550522-ffb9-4c86-84ad-1773bd2c7b7f",
   "metadata": {},
   "source": [
    "# 8. Set up files in run directory\n",
    "\n",
    "Copy the default run directory files (MOM_input, config.yaml etc) to the new run directory. This script makes some modifications to match the settings in your experiment, namely the mask table, number of cpus required, experiment name and number of x and y grid points. Currently, if you change the layout from 10,10 you'll need to manually make your changes. Otherwise, your experiment should be ready to run after running all the cells in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2940c39d-5194-42f5-8699-e169d6ebc6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(f\"cp defaults/* {rundir}\",shell = True)\n",
    "subprocess.run(f\"ln -s {path} {rundir}/inputdir\",shell=True)\n",
    "\n",
    "## Get mask table information\n",
    "ncpus = 100\n",
    "mask_table = None\n",
    "for i in os.listdir(f\"{path}\"):\n",
    "    if \"mask_table\" in i:\n",
    "        mask_table = i\n",
    "    a = mask_table.split(\".\")[1]\n",
    "    b = mask_table.split(\".\")[2].split(\"x\")\n",
    "    ncpus = int(b[0]) * int(b[1]) - int(a)\n",
    "\n",
    "\n",
    "## Modify MOM_input\n",
    "inputfile = open(f\"{rundir}/MOM_input\",'r')\n",
    "lines = inputfile.readlines()\n",
    "inputfile.close()\n",
    "for i in range(len(lines)):\n",
    "    if \"MASKTABLE\" in lines[i]:\n",
    "        if mask_table != None:\n",
    "            lines[i] = f'MASKTABLE = \"{mask_table}\"\\n'\n",
    "        else:\n",
    "            lines[i] = \"# MASKTABLE = no mask table\n",
    "    if \"NIGLOBAL\" in lines[i]:\n",
    "        lines[i] = f\"NIGLOBAL = {str(x_indices_centre[1] - x_indices_centre[0])}\\n\"\n",
    "        \n",
    "    if \"NJGLOBAL\" in lines[i]:\n",
    "        lines[i] = f\"NJGLOBAL = {str(y_indices_centre[1] - y_indices_centre[0])}\\n\"\n",
    "        \n",
    "inputfile = open(f\"{rundir}/MOM_input\",'w')\n",
    "\n",
    "inputfile.writelines(lines)\n",
    "inputfile.close()\n",
    "\n",
    "## Modify SIS_input\n",
    "inputfile = open(f\"{rundir}/SIS_input\",'r')\n",
    "lines = inputfile.readlines()\n",
    "inputfile.close()\n",
    "for i in range(len(lines)):\n",
    "    if \"MASKTABLE\" in lines[i]:\n",
    "        lines[i] = f'MASKTABLE = \"{mask_table}\"\\n'\n",
    "    if \"NIGLOBAL\" in lines[i]:\n",
    "        lines[i] = f\"NIGLOBAL = {str(x_indices_centre[1] - x_indices_centre[0])}\\n\"\n",
    "        \n",
    "    if \"NJGLOBAL\" in lines[i]:\n",
    "        lines[i] = f\"NJGLOBAL = {str(y_indices_centre[1] - y_indices_centre[0])}\\n\"\n",
    "        \n",
    "inputfile = open(f\"{rundir}/SIS_input\",'w')\n",
    "inputfile.writelines(lines)\n",
    "inputfile.close()\n",
    "\n",
    "## Modify config.yaml\n",
    "inputfile = open(f\"{rundir}/config.yaml\",'r')\n",
    "lines = inputfile.readlines()\n",
    "inputfile.close()\n",
    "for i in range(len(lines)):\n",
    "    if \"ncpus\" in lines[i]:\n",
    "        lines[i] = f'ncpus: \"{str(ncpus)}\"\\n'\n",
    "    if \"jobname\" in lines[i]:\n",
    "        lines[i] = f\"jobname: mom6_{expt_name}\\n\"\n",
    "        \n",
    "    if \"input:\" in lines[i]:\n",
    "        lines[i + 1] = f\"    - {path}\\n\"\n",
    "        \n",
    "inputfile = open(f\"{rundir}/config.yaml\",'w')\n",
    "inputfile.writelines(lines)\n",
    "inputfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db06a5f6",
   "metadata": {},
   "source": [
    "# 9. Copy the vcoord.nc file\n",
    "\n",
    "For now just copy the vertical coordinate file from an existing run. This could be generated instead to suit your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd1a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(f\"cp vcoord.nc {path}\",shell=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5946ed9b-95d3-4e9f-a155-7d4bb6d35e83",
   "metadata": {},
   "source": [
    "# Read in to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dda433-ad85-4ae2-a4e5-98ed9ee005f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_vel = xr.open_dataset(path + \"forcing/init_vel_cgrid.nc\")\n",
    "land_mask = xr.open_dataset(path + \"land_mask.nc\")\n",
    "topog = xr.open_dataset(path + \"topog.nc\")\n",
    "init_eta = xr.open_dataset(path + \"forcing/init_eta.nc\")\n",
    "\n",
    "eacpath = \"/g/data/x77/ahg157/inputs/mom6/eac-01/\"\n",
    "\n",
    "eac_init_vel = xr.open_dataset(eacpath + \"forcing/init_vel_cgrid.nc\")\n",
    "eac_init_eta = xr.open_dataset(eacpath + \"forcing/init_eta.nc\")\n",
    "\n",
    "eac_land_mask = xr.open_dataset(eacpath + \"land_mask.nc\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "filtering",
   "language": "python",
   "name": "filtering"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
