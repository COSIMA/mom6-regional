{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forcing with model output\n",
    "\n",
    "### This example is most useful for people with access to Australia's National Computational Infrastructure facility, because the model output being used is hosted here. For others, the 'reanalysis-forced' example will be more helpful as it relies only on open source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import cycle\n",
    "import os\n",
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import subprocess\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "## For NCI users, uncomment the following line if you just want to import from my copy of the code and sidestep the installation process\n",
    "## In this case just use the latest version of the analysis env. HOWEVER! Note that without the latest version of xesmf which is not yet\n",
    "## available on analysis3, the regridding will only work in serial and won't be suitable for large domains\n",
    "\n",
    "# os.chdir(\"/home/149/ab8992/cosima_regional/regional-mom6/regional_mom6/\")\n",
    "\n",
    "import regional_mom6 as rm\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this package do?\n",
    "\n",
    "Setting up a regional model in MOM6 is a pain. The goal of this package is that users should spend their debugging time fixing a model that's running and doing weird things, rather than puzzling over a model that won't even start.\n",
    "\n",
    "In running this notebook, you'll hopefully have a running MOM6 regional model. There will still be a lot of fiddling to do with the MOM_input file to make sure that the parameters are set up right for your domain, and you might want to manually edit some of the input files. BUT, this package should help you bypass most of the woes of regridding, encoding and understanding the arcane arts of the MOM6 boundary segment files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this notebook do?\n",
    "\n",
    "This notebook demonstrates how to set up a regional domain using the package. By the end you should have a running MOM6 experiment on the domain of your choice. To make a stable test case:\n",
    "\n",
    "* Avoid any regions with ice\n",
    "* Avoid regions near the north pole\n",
    "* Although the default configuration is meant to be RYF, I've not fixed up the calendar and encoding to run longer than a year just yet\n",
    "\n",
    "\n",
    "Input Type | Source\n",
    "---|---\n",
    "Surface | JRA \n",
    "Ocean | ACCESS OM2-01\n",
    "Bathymetry | Gebco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Your personal environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch = \"/scratch/v45/ab8992\"\n",
    "home = \"/home/149/ab8992\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Choose our domain, define workspace paths\n",
    "\n",
    "To make sure that things are working I'd recommend starting with the default example defined below. If this runs ok, then change to a domain of your choice and hopefully it runs ok too! There's some troubleshooting you can do if not (check readme / readthedocs)\n",
    "\n",
    "To find the lat/lon of the domain you want to test you can use <a href=\"https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/download\" > this GUI </a> and copy paste below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_name = \"tasmania-om2test\"\n",
    "\n",
    "## Choose your coordinates and the name of your experiment\n",
    "yextent = [-48,-38.95] ## latitude\n",
    "xextent = [143,150] ## longitude\n",
    "\n",
    "daterange = [\"1990-01-01 00:00:00\", \"1990-01-05 00:00:00\"] ## 2003 is a good compromise for GLORYs and JRA forcing as they overlap. JRA ends in 2012, GLORYS starts in 1993\n",
    "\n",
    "## Place where all your input files go\n",
    "inputdir = f\"{scratch}/regional_mom6_configs/{expt_name}/\"\n",
    "\n",
    "## Directory where you'll run the experiment from\n",
    "rundir = f\"{home}/mom6_rundirs/{expt_name}/\"\n",
    "\n",
    "## Directory where fre tools are stored\n",
    "toolpath = \"/home/157/ahg157/repos/mom5/src/tools/\" ## Compiled tools needed for construction of mask tables\n",
    "\n",
    "## Directory where raw downloads go before processing\n",
    "tmpdir = f\"{scratch}/regional_tmp/{expt_name}\"\n",
    "\n",
    "for i in [rundir,tmpdir,inputdir]:\n",
    "    if not os.path.exists(i):\n",
    "        subprocess.run(f\"mkdir {i} -p\",shell=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare ocean forcing data\n",
    "\n",
    "We need to cut out our ocean forcing. The pipeline expects an initial condition and one time-dependent segment per non-land boundary. Naming convention is \"east_unprocessed\" and \"ic_unprocessed\" for initial condition. The following provides an example for cutting out the necessary forcing files from an ocean model. It's hardcoded to pull data from a Repeat Year Forced ACCESS-OM2-01 database, but you should be able to recycle parts of the code to cut out data from a dataset of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you have access to where it's located on Gadi, you can execute the following cell to cut out and save your segments and use these instead. The default I've set it at below is to cut out 3 months. To cut out a year, uncomment the code above which concatenates several input files together. Keep in mind that these input files are HUGE and they'll take a while to open and processes. To do a whole year, you'll want to run with a whole node and go make yourself a cup of coffee (and maybe read the paper for a bit). \n",
    "\n",
    "The advantage of doing this though is that the input files that the pipeline has to deal with are a lot smaller, making subsequent computation a lot quicker. An older iteration of the boundary brushcutter was to read data directly from the huge datasets, but this required some very careful chunking to not break your kernel. \n",
    "\n",
    "**NOTE: I haven't automated this properly and it's hardcoded for the year of 1990, which corresponds to files 1077 - 1082. Could maybe use COSIMA cookbook for this step instead?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########## TWO OPTIONS: ############################\n",
    "\n",
    "## Use this if you want to do a quick test for up to 3 months\n",
    "om2_input = xr.open_mfdataset(f\"/g/data/ik11/outputs/access-om2-01/01deg_jra55v13_ryf9091/output1077/ocean/ocean_daily*\",parallel=True,chunks='auto')[[\"u\",\"v\",\"salt\",\"temp\",\"eta_t\"]].sel(    \n",
    "    yu_ocean = slice(yextent[0] - 0.2,yextent[1] + 0.2),\n",
    "    yt_ocean = slice(yextent[0] - 0.2,yextent[1] + 0.2)\n",
    ").isel(time = slice(0,5))\n",
    "## Use this to cut out entire year \n",
    "# om2_input = xr.concat(\n",
    "#     [xr.open_mfdataset(f\"/g/data/ik11/outputs/access-om2-01/01deg_jra55v13_ryf9091/output{i}/ocean/ocean_daily*\",decode_times = False,parallel=True,chunks='auto') for i in range(1077,1082)],\n",
    "#     \"time\"\n",
    "# )\n",
    "#!  for i in range(1077,1082) is hardcoded to choose the year of 1990 Jan -> Dec 31. \n",
    "#######################################################\n",
    "\n",
    "# Cut out initial condition and save\n",
    "ic = om2_input.isel(time = 0)\n",
    "\n",
    "## Nicer Slicer handles seams in longitude and different grids. Ensures that the output matches our 'xextent'\n",
    "ic = rm.nicer_slicer(ic,[xextent[0],xextent[1]],[\"xu_ocean\",\"xt_ocean\"])\n",
    "ic.to_netcdf(tmpdir + \"/ic_unprocessed\")\n",
    "\n",
    "## Cut out East and West segments. Does lat slice first then uses nicer slicer for lon slice\n",
    "eastwest = om2_input.sel(    \n",
    "    yu_ocean = slice(yextent[0] - 0.2,yextent[1] + 0.2),\n",
    "    yt_ocean = slice(yextent[0] - 0.2,yextent[1] + 0.2)\n",
    ")\n",
    "rm.nicer_slicer(eastwest,[xextent[1],xextent[1]],[\"xu_ocean\",\"xt_ocean\"]).to_netcdf(tmpdir + \"/east_unprocessed\")\n",
    "rm.nicer_slicer(eastwest,[xextent[0],xextent[0]],[\"xu_ocean\",\"xt_ocean\"]).to_netcdf(tmpdir + \"/west_unprocessed\")\n",
    "\n",
    "## Cut out North and South segments\n",
    "northsouth = rm.nicer_slicer(om2_input,[xextent[0],xextent[1]],[\"xu_ocean\",\"xt_ocean\"])\n",
    "northsouth.sel(\n",
    "    yu_ocean = slice(yextent[1] - 0.2,yextent[1] + 0.2),\n",
    "    yt_ocean = slice(yextent[1] - 0.2,yextent[1] + 0.2)\n",
    ").to_netcdf(tmpdir + \"/north_unprocessed\")\n",
    "northsouth.sel(\n",
    "    yu_ocean = slice(yextent[0] - 0.2,yextent[0] + 0.2),\n",
    "    yt_ocean = slice(yextent[0] - 0.2,yextent[0] + 0.2)\n",
    ").to_netcdf(tmpdir + \"/south_unprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Make experiment object\n",
    "This object keeps track of your domain basics, as well as generating the hgrid, vgrid and setting up the folder structures. \n",
    "\n",
    "After running you can have a look at your grids by calling `expt.hgrid` and `expt.vgrid`\n",
    "\n",
    "Plotting vgrid with marker = '.' option lets you see the spacing, or plotting \n",
    "```python\n",
    "np.diff(expt.hgrid.zl).plot(marker = '.')\n",
    "```\n",
    " shows you the vertical spacing profile.\n",
    "\n",
    "### Modular workflow!\n",
    "\n",
    "After constructing your expt object, if you don't like my lazy default hgrid and vgrid you can simply modify and overwrite them. However, you'll also need to save them to disk again as I've not automated this just yet. For example:\n",
    "\n",
    "```python\n",
    "expt.hgrid = custom_hgrid\n",
    "expt.hgrid.to_netcdf(f\"{inputdir}/hgrid.nc\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ml)\n",
    "expt = rm.experiment(\n",
    "    xextent,\n",
    "    yextent,\n",
    "    daterange,\n",
    "    0.05,  # Resolution\n",
    "    75,    # Number of vertical layers\n",
    "    10,    # Ratio of largest to smallest vertical layer. Select 1 for linear, negative number for higher resolution at bottom\n",
    "    4500,  # Depth of simulation\n",
    "    rundir,\n",
    "    inputdir,\n",
    "    toolpath\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up bathymetry\n",
    "\n",
    "Similarly to ocean forcing, we point our 'bathymetry' method at the location of the file of choice, and pass it a dictionary mapping variable names. This time we don't need to preprocess the topography since it's just a 2D field and easier to deal with. Afterwards you can run `expt.topog` and have a look at your domain. After running this cell, your input directory will contain other topography - adjacent things like the ocean mosaic and mask table too. This defaults to a 10x10 layout which can be updated later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.bathymetry(\n",
    "    '/g/data/ik11/inputs/GEBCO_2022/GEBCO_2022.nc',\n",
    "    {\"xh\":\"lon\",\n",
    "     \"yh\":\"lat\",\n",
    "     \"elevation\":\"elevation\"}, ## Again this dictionary just maps mom6 variable names to what they are in your topog.\n",
    "     minimum_layers = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out your domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.topog.depth.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 5: Handle the ocean forcing - where the magic happens\n",
    "\n",
    "This cuts out and interpolates the initial condition as well as all boundaries (unless you don't pass it boundaries).\n",
    "\n",
    "The dictionary maps the MOM6 variable names to what they're called in your ocean input file. Notice how the horizontal dimensions are x and y, vs xh, yh, xq, yq. This is because ACCESS-OM2-01 is on a `B` grid, so we need to differentiate between `q` and `t` points. \n",
    "\n",
    "If one of your segments is land, you can delete its string from the 'boundaries' list. You'll need to update MOM_input to reflect this though so it knows how many segments to look for, and their orientations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR ACCESS OM2: \n",
    "expt.ocean_forcing(\n",
    "    tmpdir,  ## Path to ocean foring files\n",
    "    {\"time\":\"time\",\n",
    "     \"yh\":\"yt_ocean\",\n",
    "     \"xh\":\"xt_ocean\",\n",
    "     \"xq\":\"xu_ocean\",\n",
    "     \"yq\":\"yu_ocean\",\n",
    "     \"zl\":\"st_ocean\",\n",
    "     \"eta\":\"eta_t\",\n",
    "     \"u\":\"u\",\n",
    "     \"v\":\"v\",\n",
    "     \"tracers\":{\"salt\":\"salt\",\"temp\":\"temp\"}},\n",
    "    boundaries = [\"south\",\"north\",\"west\",\"east\"],\n",
    "    gridtype=\"B\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 Run the FRE tools\n",
    "\n",
    "This is just a wrapper for the FRE tools needed to make the mosaics and masks for the experiment. The only thing you need to tell it is the processor layout. In this case we're saying that we want a 10 by 10 grid of 100 processors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.FRE_tools((10,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Modify the default input directory to make a (hopefully) runnable configuration out of the box\n",
    "\n",
    "This cell just copies a default run directory and modifies it to match your configuration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(f\"cp default_rundir/jra_surface/* {rundir} -r\",shell = True)\n",
    "subprocess.run(f\"ln -s {inputdir} {rundir}/inputdir\",shell=True)\n",
    "\n",
    "hgrid = xr.open_dataset(f\"{inputdir}/hgrid.nc\")\n",
    "\n",
    "## Get mask table information\n",
    "ncpus = 10\n",
    "mask_table = None\n",
    "for i in os.listdir(f\"{inputdir}\"):\n",
    "    if \"mask_table\" in i:\n",
    "        mask_table = i\n",
    "        a = mask_table.split(\".\")[1]\n",
    "        b = mask_table.split(\".\")[2].split(\"x\")\n",
    "        ncpus = int(b[0]) * int(b[1]) - int(a)\n",
    "\n",
    "\n",
    "## Modify MOM_input\n",
    "inputfile = open(f\"{rundir}/MOM_input\",'r')\n",
    "lines = inputfile.readlines()\n",
    "inputfile.close()\n",
    "for i in range(len(lines)):\n",
    "    if \"MASKTABLE\" in lines[i]:\n",
    "        if mask_table != None:\n",
    "            lines[i] = f'MASKTABLE = \"{mask_table}\"\\n'\n",
    "        else:\n",
    "            lines[i] = \"# MASKTABLE = no mask table\"\n",
    "    if \"LAYOUT =\" in lines[i] and \"IO\" not in lines[i]:\n",
    "        lines[i] = f'LAYOUT = {expt.layout[1]},{expt.layout[0]}\\n'\n",
    "\n",
    "    if \"NIGLOBAL\" in lines[i]: \n",
    "        # lines[i] = f\"NIGLOBAL = {str(x_indices_centre[1] - x_indices_centre[0])}\\n\"\n",
    "        lines[i] = f\"NIGLOBAL = {hgrid.nx.shape[0]//2}\\n\"\n",
    "\n",
    "        \n",
    "    if \"NJGLOBAL\" in lines[i]:\n",
    "        # lines[i] = f\"NJGLOBAL = {str(y_indices_centre[1] - y_indices_centre[0])}\\n\"\n",
    "        lines[i] = f\"NJGLOBAL = {hgrid.ny.shape[0]//2}\\n\"\n",
    "\n",
    "        \n",
    "inputfile = open(f\"{rundir}/MOM_input\",'w')\n",
    "\n",
    "inputfile.writelines(lines)\n",
    "inputfile.close()\n",
    "\n",
    "## Modify SIS_input\n",
    "inputfile = open(f\"{rundir}/SIS_input\",'r')\n",
    "lines = inputfile.readlines()\n",
    "inputfile.close()\n",
    "for i in range(len(lines)):\n",
    "    if \"MASKTABLE\" in lines[i]:\n",
    "        lines[i] = f'MASKTABLE = \"{mask_table}\"\\n'\n",
    "    if \"NIGLOBAL\" in lines[i]:\n",
    "        # lines[i] = f\"NIGLOBAL = {str(x_indices_centre[1] - x_indices_centre[0])}\\n\"\n",
    "        lines[i] = f\"NIGLOBAL = {hgrid.nx.shape[0]//2}\\n\"\n",
    "    if \"LAYOUT =\" in lines[i] and \"IO\" not in lines[i]:\n",
    "        lines[i] = f'LAYOUT = {expt.layout[1]},{expt.layout[0]}\\n'\n",
    "    if \"NJGLOBAL\" in lines[i]:\n",
    "        # lines[i] = f\"NJGLOBAL = {str(y_indices_centre[1] - y_indices_centre[0])}\\n\"\n",
    "        lines[i] = f\"NJGLOBAL = {hgrid.ny.shape[0]//2}\\n\"\n",
    "        \n",
    "inputfile = open(f\"{rundir}/SIS_input\",'w')\n",
    "inputfile.writelines(lines)\n",
    "inputfile.close()\n",
    "\n",
    "## Modify config.yaml \n",
    "inputfile = open(f\"{rundir}/config.yaml\",'r')\n",
    "lines = inputfile.readlines()\n",
    "inputfile.close()\n",
    "for i in range(len(lines)):\n",
    "    if \"ncpus\" in lines[i]:\n",
    "        lines[i] = f'ncpus: {str(ncpus)}\\n'\n",
    "    if \"jobname\" in lines[i]:\n",
    "        lines[i] = f\"jobname: mom6_{expt_name}\\n\"\n",
    "        \n",
    "    if \"input:\" in lines[i]:\n",
    "        lines[i + 1] = f\"    - {inputdir}\\n\"\n",
    "\n",
    "inputfile = open(f\"{rundir}/config.yaml\",'w')\n",
    "inputfile.writelines(lines)\n",
    "inputfile.close()\n",
    "\n",
    "\n",
    "# Modify input.nml \n",
    "inputfile = open(f\"{rundir}/input.nml\",'r')\n",
    "lines = inputfile.readlines()\n",
    "inputfile.close()\n",
    "for i in range(len(lines)):\n",
    "    if \"current_date\" in lines[i]:\n",
    "        tmp = daterange[0].split(\" \")[0].split(\"-\")\n",
    "        lines[i] = f\"{lines[i].split(' = ')[0]} = {int(tmp[0])},{int(tmp[1])},{int(tmp[2])},0,0,0,\\n\"\n",
    "\n",
    " \n",
    "inputfile = open(f\"{rundir}/input.nml\",'w')\n",
    "inputfile.writelines(lines)\n",
    "inputfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Run your model!\n",
    "\n",
    "To do this, navigate to your run directory in terminal. If you're working on NCI, you can do this via:\n",
    "\n",
    "```\n",
    "module load conda/analysis3\n",
    "payu setup -f\n",
    "payu run -f\n",
    "```\n",
    "\n",
    "By default `input.nml` is set to only run for 5 days as a test. If this is successful, you can modify this file to then run for longer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 and beyond: Fiddling, troubleshooting and fine tuning\n",
    "\n",
    "Hopefully your model is running. If not, the first thing you should do is reduce the timestep. You can do this by adding `#override DT=XXXX` to your `MOM_override` file. \n",
    "\n",
    "If there's strange behaviour on your boundaries, you could play around with the `nudging timescale` (an example is already included in the `MOM_override` file). Sometimes, if your boundary has a lot going on (like all of the eddies spinning off the ACC), it can be hard to avoid these edge effects. This is because the chaotic, submesoscale structures developed within the regional domain won't match those at the boundary. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
