{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Regional Tasmania forced by Reanalysis dataset and ERA5\n",
    "\n",
    "**Before you begin, make sure you've downloaded and installed the package, and have set up your FRE-NC tools as outlined in the package README**\n",
    "\n",
    "In addition, for this example you'll need a copy of the [GEBCO bathymetry](https://www.gebco.net/data_and_products/gridded_bathymetry_data/), access to the [GLORYs ocean reanalysis data](https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/description), and [ERA5 surface forcing for 2003](https://www.ecmwf.int/en/forecasts/dataset/ecmwf-reanalysis-v5). \n",
    "\n",
    "This script is designed to read in the entire global extent of ERA5 and GEBCO, so you don't need to worry about cutting it down to size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this notebook do?\n",
    "This notebook is designed to set you up with a working MOM6 regional configuration. First, try and get it running with our default Tasmania case, then you can clone the notebook and modify for your region of interest. \n",
    "\n",
    "Input Type | Source | Subsets required\n",
    "---|---|---\n",
    "Surface | [ERA5 surface forcing](https://www.ecmwf.int/en/forecasts/dataset/ecmwf-reanalysis-v5) | Data from 2003, whole globe or subset around domain\n",
    "Ocean | [GLORYs reanalysis product](https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/description) | Boundary segments & initial condition. See section 2 for details.   \n",
    "Bathymetry | [GEBCO](https://www.gebco.net/data_and_products/gridded_bathymetry_data/) | whole globe or subset around domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regional_mom6 as rm\n",
    "from pathlib import Path\n",
    "from dask.distributed import Client\n",
    "client = Client() # Start a dask cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Choose our domain, define workspace paths\n",
    "\n",
    "To make sure that things are working I'd recommend starting with the default example defined below. If this runs ok, then change to a domain of your choice and hopefully it runs ok too! There's some troubleshooting you can do if not (check readme / readthedocs)\n",
    "\n",
    "To find the lat/lon of the domain you want to test you can use <a href=\"https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/download\" > this GUI </a> and copy paste below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_name = \"tasmania-example-reanalysis\"\n",
    "\n",
    "latitude_extent = [-48, -38.95]\n",
    "longitude_extent = [143, 150]\n",
    "\n",
    "date_range = [\"2003-01-01 00:00:00\", \"2003-01-05 00:00:00\"]\n",
    "\n",
    "## Place where all your input files go \n",
    "input_dir = Path(f\"mom6_input_directories/{expt_name}/\")\n",
    "\n",
    "## Directory where you'll run the experiment from\n",
    "run_dir = Path(f\"mom6_run_directories/{expt_name}/\")\n",
    "\n",
    "## Directory where fre tools are stored \n",
    "toolpath_dir = Path(\"PATH_TO_FRE_TOOLS\") ## Compiled tools needed for construction of mask tables\n",
    "\n",
    "## Path to where your raw ocean forcing files are stored\n",
    "glorys_path = Path(\"PATH_TO_GLORYS_DATA\" )\n",
    "\n",
    "for i in [rundir,glorys_path,inputdir]:\n",
    "    if not os.path.exists(str(i)):\n",
    "        os.makedirs(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare ocean forcing data\n",
    "\n",
    "We need to cut out our ocean forcing. The package expects an initial condition and one time-dependent segment per non-land boundary. Naming convention is \"east_unprocessed\" for segments and \"ic_unprocessed\" for the initial condition.\n",
    "\n",
    "Data can be downloaded directly from the [Copernicus Marine data store](https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/download) via their GUI (once you're logged in). Unfortunately their old client `motuclient` is no longer working and they're currently in the process of replacing it. Until this is restored, and this notebook is updated with their new client, users will need to download each segment manually\n",
    "\n",
    "1. Using the GUI, select an area matching your xextent and yextent for the first day in your daterange. Download and label `ic_unprocessed`, then store it in your `glorys_path` folder.\n",
    "2. Using the GUI Select the Eastern boundary of your domain (if you have one that contains ocean). Give a buffer of ~0.5 degrees in all directions, and download for your full daterange. Download and label `east_unprocessed`\n",
    "3. Repeat for your other sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Make experiment object\n",
    "This object keeps track of your domain basics, as well as generating the hgrid, vgrid and setting up the folder structures. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt = rm.experiment(\n",
    "    longitude_extent = longitude_extent,\n",
    "    latitude_extent = latitude_extent,\n",
    "    date_range = date_range,\n",
    "    resolution = 0.05,\n",
    "    number_vertical_layers = 75,\n",
    "    layer_thickness_ratio = 10,\n",
    "    depth = 4500,\n",
    "    mom_run_dir = run_dir,\n",
    "    mom_input_dir = input_dir,\n",
    "    toolpath_dir = toolpath_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running you can have a look at your grids by calling `expt.hgrid` and `expt.vgrid`\n",
    "\n",
    "Plotting vgrid with marker = '.' option lets you see the spacing, or plotting \n",
    "```python\n",
    "np.diff(expt.vgrid.zl).plot(marker = '.')\n",
    "```\n",
    " shows you the vertical spacing profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modular workflow!\n",
    "\n",
    "After constructing your expt object, if you don't like the default hgrid and vgrids you can simply modify and overwrite them. However, you'll then also need to save them to disk again. For example:\n",
    "\n",
    "```python\n",
    "new_hgrid = xr.open_dataset(inputdir / \"hgrid.nc\")\n",
    "```\n",
    "Modify `new_hgrid`, ensuring that metadata is retained to keep MOM6 happy. Then, save your changes\n",
    "\n",
    "```python\n",
    "expt.hgrid = new_hgrid\n",
    "\n",
    "expt.hgrid.to_netcdf(inputdir / \"hgrid.nc\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up bathymetry\n",
    "\n",
    "Similarly to ocean forcing, we point our 'bathymetry' method at the location of the file of choice, and pass it a dictionary mapping variable names. This time we don't need to preprocess the topography since it's just a 2D field and easier to deal with. Afterwards you can run `expt.topog` and have a look at your domain. After running this cell, your input directory will contain other topography - adjacent things like the ocean mosaic and mask table too. This defaults to a 10x10 layout which can be updated later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.bathymetry(\n",
    "    'PATH_TO_GEBCO_FILE/GEBCO_2022.nc', \n",
    "    {\"xh\":\"lon\",\n",
    "     \"yh\":\"lat\",\n",
    "     \"elevation\":\"elevation\"}, ## Again this dictionary just maps mom6 variable names to what they are in your topog.\n",
    "     minimum_layers = 1        ## Minimum number of layers allowed. Any areas with fewer layers are marked as land\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out your domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbval-ignore-output",
     "nbval-skip"
    ]
   },
   "outputs": [],
   "source": [
    "expt.topog.depth.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 5: Handle the ocean forcing - where the magic happens\n",
    "\n",
    "This cuts out and interpolates the initial condition as well as all boundaries (unless you don't pass it boundaries).\n",
    "\n",
    "The dictionary maps the MOM6 variable names to what they're called in your ocean input file. Notice how for GLORYs, the horizontal dimensions are x and y, vs xh, yh, xq, yq for ACCESS OM2-01. This is because for an 'A' grid type tracers share the grid with velocities so there's no difference.\n",
    "\n",
    "If one of your segments is land, you can delete its string from the 'boundaries' list. You'll need to update MOM_input to reflect this though so it knows how many segments to look for, and their orientations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping from the GLORYS variables and dimensions to the MOM6 ones\n",
    "ocean_varnames =     {\"time\":\"time\",\n",
    "     \"y\":\"latitude\",\n",
    "     \"x\":\"longitude\",\n",
    "     \"zl\":\"depth\",\n",
    "     \"eta\":\"zos\",\n",
    "     \"u\":\"uo\",\n",
    "     \"v\":\"vo\",\n",
    "     \"tracers\":{\"salt\":\"so\",\n",
    "                \"temp\":\"thetao\"\n",
    "                }\n",
    "    }\n",
    "\n",
    "# Set up the initial condition\n",
    "expt.initial_condition(\n",
    "    glorys_path / \"ic_unprocessed.nc\",     # The directory where the unprocessed initial condition is stored, as definied earlier\n",
    "    ocean_varnames,\n",
    "    gridtype=\"A\"\n",
    "    )\n",
    "\n",
    "# Now iterate through our four boundaries \n",
    "for i,orientation in enumerate([\"south\",\"north\",\"west\",\"east\"]):\n",
    "    expt.rectangular_boundary(\n",
    "        glorys_path / (orientation + \"_unprocessed.nc\"),\n",
    "        ocean_varnames,\n",
    "        orientation,    # Needs to know the cardinal direction of the boundary\n",
    "        i + 1,          # Just a number to identify the boundary. Indexes from 1 \n",
    "        gridtype=\"A\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run the FRE tools\n",
    "\n",
    "This is just a wrapper for the FRE tools needed to make the mosaics and masks for the experiment. The only thing you need to tell it is the processor layout. In this case we're saying that we want a 10 by 10 grid of 100 processors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.FRE_tools(layout=(10,10)) ## Here the tuple defines the processor layout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Set up ERA5 forcing:\n",
    "Here we assume you've already got ERA5 data stored somewhere on your system. \n",
    "\n",
    "For this example, we are forcing for the entire year of 2003 so we just generate a single forcing file with 2003's data.\n",
    "\n",
    "Below is a table showing ERA5 characteristics and what needs to be done to sort it out\n",
    "### Required ERA data:\n",
    "Name | ERA filename | era variable name | Units\n",
    "---|---|---|---\n",
    "Surface Pressure | sp | sp | Pa \n",
    "Surface Temperature | 2t | t2m | K \n",
    "Meridional Wind | 10v | v10 | m/s \n",
    "Zonal Wind | 10u | u10 | m/s \n",
    "Specific Humidity | na | na | kg/kg, calculated from dewpoint temperature\n",
    "Dewpoint Temperature | 2d | d2m | K\n",
    "\n",
    "\n",
    "We calculate specific humidity $q$ from dewpoint temperature $T_d$ and surface pressure $P$ via saturation vapour pressure $P_v$.\n",
    "\n",
    "$\\large P_v = 10^{8.07131 - \\frac{1730.63}{233.426 + T}} \\frac{101325}{760} $ Pascals\n",
    "\n",
    "$\\large q = 0.001 * 0.622  \\frac{P_v}{P}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [],
   "source": [
    "expt.setup_era5(\"PATH_TO_ERA5_DATA/era5/single-levels/reanalysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Modify the default input directory to make a (hopefully) runnable configuration out of the box\n",
    "\n",
    "This step copies the default directory, and modifies the `MOM_layout` files to match your experiment by inserting the right number of x,y points and cpu layout. If you use Payu to run mom6, set the `using_payu` flag to `True` and an example `config.yaml` file will be copied to your run directory. This still needs to be modified manually to work with your projects, executable etc.\n",
    "\n",
    "You also need to pass the path to where you git cloned the mom6-regional code. This just ensures that the funciton can find the premade run directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.setup_run_directory(\"PATH_TO_REGIONAL_MOM6_CODE\",surface_forcing = \"era5\",using_payu = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Run and Troubleshoot!\n",
    "\n",
    "To do this, navigate to your run directory in the terminal, and use your favourite tool to run the experiment on your system. \n",
    "\n",
    "Hopefully your model is running. If not, the first thing you should do is reduce the timestep. You can do this by adding `#override DT=XXXX` to your `MOM_override` file. \n",
    "\n",
    "If there's strange behaviour on your boundaries, you could play around with the `nudging timescale` (an example is already included in the `MOM_override` file). Sometimes, if your boundary has a lot going on (like all of the eddies spinning off the ACC), it can be hard to avoid these edge effects. This is because the chaotic, submesoscale structures developed within the regional domain won't match those at the boundary. \n",
    "\n",
    "Another thing that can go wrong is little bays creating non-advective cells at your boundaries. Keep an eye out for tiny bays where one side is taken up by a boundary segment. You can either fill them in manually, or move your boundary slightly to avoid them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
